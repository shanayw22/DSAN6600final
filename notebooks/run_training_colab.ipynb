{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Translation Quality Estimation Model Training - Colab Runner\n",
        "\n",
        "This notebook runs the `train_model.py` script on Google Colab with GPU support.\n",
        "\n",
        "## Features:\n",
        "- ‚úÖ Automatic GPU detection and configuration\n",
        "- ‚úÖ Configurable sample size\n",
        "- ‚úÖ Optimized for Colab's GPU environment\n",
        "- ‚úÖ Easy to use with minimal setup\n",
        "\n",
        "## Instructions:\n",
        "1. Upload your data files to Colab or mount Google Drive\n",
        "2. Set `SAMPLE_SIZE` variable below\n",
        "3. Run all cells\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Configuration\n",
        "# ============================================================================\n",
        "\n",
        "# Set the number of sentence pairs to use for training\n",
        "# Use None for full dataset, or a number like 200000 for a sample\n",
        "SAMPLE_SIZE = 2000  # Change this value as needed\n",
        "\n",
        "# Mount Google Drive and change to project directory\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "print(\"üìÇ Mounting Google Drive...\")\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "print(\"üìÅ Changing to project directory...\")\n",
        "os.chdir('/content/drive/MyDrive/DSAN6600final')\n",
        "\n",
        "# Data path - now relative to project directory\n",
        "DATA_PATH = 'data/hi-zh.txt/'\n",
        "\n",
        "# Get current working directory for reference\n",
        "PROJECT_DIR = os.getcwd()\n",
        "\n",
        "print(f\"\\n‚úÖ Configuration:\")\n",
        "print(f\"  üìä SAMPLE_SIZE: {SAMPLE_SIZE if SAMPLE_SIZE else 'Full dataset'}\")\n",
        "print(f\"  üìÅ DATA_PATH: {DATA_PATH}\")\n",
        "print(f\"  üìÇ PROJECT_DIR: {PROJECT_DIR}\")\n",
        "print(f\"  üíæ OUTPUT_DIR: models/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Install Dependencies\n",
        "# ============================================================================\n",
        "\n",
        "print(\"üì¶ Installing dependencies...\")\n",
        "print(\"  ‚Üí Installing core packages...\")\n",
        "%pip install -q pandas>=1.5.0 numpy>=1.23.0 scikit-learn>=1.2.0\n",
        "print(\"  ‚Üí Installing ML libraries...\")\n",
        "%pip install -q sentence-transformers>=2.2.0\n",
        "%pip install -q tensorflow>=2.10.0\n",
        "print(\"  ‚Üí Installing transformers and PyTorch...\")\n",
        "%pip install -q transformers>=4.20.0 torch>=1.12.0\n",
        "print(\"  ‚Üí Installing utilities...\")\n",
        "%pip install -q scipy>=1.9.0\n",
        "%pip install -q tqdm  # Progress bars\n",
        "\n",
        "print(\"\\n‚úÖ All dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Check GPU Availability\n",
        "# ============================================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Check for GPU\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(f\"‚úì GPU detected: {len(gpus)} GPU(s)\")\n",
        "    for i, gpu in enumerate(gpus):\n",
        "        print(f\"  GPU {i}: {gpu.name}\")\n",
        "    # Enable memory growth\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"‚úì GPU memory growth enabled\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"  Warning: {e}\")\n",
        "else:\n",
        "    print(\"‚ö† No GPU detected. Training will be slower on CPU.\")\n",
        "    print(\"  To enable GPU in Colab: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
        "\n",
        "# Check TensorFlow version\n",
        "print(f\"\\nTensorFlow version: {tf.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Setup Project Structure\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# We're already in the project directory, so use relative paths\n",
        "# Create models directory if it doesn't exist\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# Add scripts directory to path (relative to current directory)\n",
        "scripts_dir = os.path.join(os.getcwd(), 'scripts')\n",
        "sys.path.insert(0, scripts_dir)\n",
        "\n",
        "print(\"‚úÖ Project structure verified\")\n",
        "print(f\"  üìÇ Working directory: {os.getcwd()}\")\n",
        "print(f\"  üìÅ Scripts directory: {scripts_dir}\")\n",
        "print(f\"  üíæ Models directory: {os.path.join(os.getcwd(), 'models')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Upload Data Files (if not already uploaded)\n",
        "# ============================================================================\n",
        "\n",
        "# Check if data files exist\n",
        "data_files = [\n",
        "    'CCMatrix.hi-zh.hi',\n",
        "    'CCMatrix.hi-zh.zh',\n",
        "    'CCMatrix.hi-zh.scores'\n",
        "]\n",
        "\n",
        "data_path = Path(DATA_PATH)\n",
        "all_exist = all((data_path / f).exists() for f in data_files)\n",
        "\n",
        "if all_exist:\n",
        "    print(f\"‚úì Data files found at: {DATA_PATH}\")\n",
        "    for f in data_files:\n",
        "        file_path = data_path / f\n",
        "        if file_path.exists():\n",
        "            size_mb = file_path.stat().st_size / (1024 * 1024)\n",
        "            print(f\"  ‚úì {f} ({size_mb:.2f} MB)\")\n",
        "else:\n",
        "    print(\"‚ö† Data files not found. Please upload them:\")\n",
        "    print(\"  Option 1: Use Colab's file upload (Files ‚Üí Upload)\")\n",
        "    print(\"  Option 2: Mount Google Drive and copy files\")\n",
        "    print(\"  Option 3: Use wget/curl to download from URL\")\n",
        "    print(f\"\\n  Expected location: {DATA_PATH}\")\n",
        "    print(f\"  Required files: {', '.join(data_files)}\")\n",
        "    \n",
        "    # Option: Upload files interactively\n",
        "    from google.colab import files\n",
        "    print(\"\\nüì§ Upload data files now? (Uncomment the line below)\")\n",
        "    # uploaded = files.upload()  # Uncomment to enable file upload\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Verify train_model.py exists and modify for Colab\n",
        "# ============================================================================\n",
        "\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "# Script path relative to project directory\n",
        "script_path = 'scripts/train_model.py'\n",
        "\n",
        "# Check if script exists\n",
        "if os.path.exists(script_path):\n",
        "    print(f\"‚úÖ Found train_model.py at: {os.path.abspath(script_path)}\")\n",
        "    \n",
        "    # Read the script\n",
        "    with open(script_path, 'r', encoding='utf-8') as f:\n",
        "        script_content = f.read()\n",
        "    \n",
        "    # Modify the script to use correct paths and SAMPLE_SIZE\n",
        "    print(\"\\nüìù Modifying script for Colab environment...\")\n",
        "    \n",
        "    # Replace DATA_PATH (handle both '../data/hi-zh.txt/' and relative paths)\n",
        "    script_content = re.sub(\n",
        "        r\"DATA_PATH = ['\\\"].*?['\\\"]\",\n",
        "        f\"DATA_PATH = '{DATA_PATH}'\",\n",
        "        script_content\n",
        "    )\n",
        "    \n",
        "    # Replace OUTPUT_DIR (handle both '../models/' and relative paths)\n",
        "    script_content = re.sub(\n",
        "        r\"OUTPUT_DIR = ['\\\"].*?['\\\"]\",\n",
        "        \"OUTPUT_DIR = 'models/'\",\n",
        "        script_content\n",
        "    )\n",
        "    \n",
        "    # Replace SAMPLE_SIZE (find the line and replace)\n",
        "    script_content = re.sub(\n",
        "        r'SAMPLE_SIZE = \\d+|SAMPLE_SIZE = None',\n",
        "        f'SAMPLE_SIZE = {SAMPLE_SIZE if SAMPLE_SIZE else None}',\n",
        "        script_content\n",
        "    )\n",
        "    \n",
        "    # Write modified script back\n",
        "    with open(script_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(script_content)\n",
        "    \n",
        "    print(\"‚úÖ Script modified for Colab\")\n",
        "    print(f\"  üìÅ DATA_PATH: {DATA_PATH}\")\n",
        "    print(f\"  üíæ OUTPUT_DIR: models/\")\n",
        "    print(f\"  üìä SAMPLE_SIZE: {SAMPLE_SIZE if SAMPLE_SIZE else 'Full dataset'}\")\n",
        "else:\n",
        "    print(\"‚ùå train_model.py not found!\")\n",
        "    print(f\"  Expected location: {os.path.abspath(script_path)}\")\n",
        "    print(\"  Please ensure the script is in the scripts/ directory\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Verify Training Configuration\n",
        "# ============================================================================\n",
        "\n",
        "# Script path relative to project directory\n",
        "script_path = 'scripts/train_model.py'\n",
        "\n",
        "if os.path.exists(script_path):\n",
        "    print(\"‚úÖ train_model.py found. Ready to execute.\")\n",
        "    print(f\"\\nüìä Training Configuration:\")\n",
        "    print(f\"  üìä Sample Size: {SAMPLE_SIZE if SAMPLE_SIZE else 'Full dataset'}\")\n",
        "    print(f\"  üìÅ Data Path: {os.path.abspath(DATA_PATH)}\")\n",
        "    print(f\"  üíæ Output Directory: {os.path.abspath('models/')}\")\n",
        "    print(f\"  üñ•Ô∏è  GPU Available: {len(gpus) > 0 if 'gpus' in locals() else 'Checking...'}\")\n",
        "    print(f\"  üìÇ Working Directory: {os.getcwd()}\")\n",
        "else:\n",
        "    print(\"‚ö† train_model.py not found. Please ensure it's in the scripts/ directory\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Run Training Script\n",
        "# ============================================================================\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "# Script path relative to project directory\n",
        "script_path = 'scripts/train_model.py'\n",
        "abs_script_path = os.path.abspath(script_path)\n",
        "\n",
        "if os.path.exists(script_path):\n",
        "    print(\"=\" * 70)\n",
        "    print(\"üöÄ STARTING TRAINING\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"üìÖ Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"üìä Sample Size: {SAMPLE_SIZE if SAMPLE_SIZE else 'Full dataset'}\")\n",
        "    print(f\"üìÅ Data Path: {os.path.abspath(DATA_PATH)}\")\n",
        "    print(f\"üíæ Output Directory: {os.path.abspath('models/')}\")\n",
        "    print(f\"üìÇ Working Directory: {os.getcwd()}\")\n",
        "    print(\"=\" * 70)\n",
        "    print()\n",
        "    \n",
        "    # Run the script from project root (script uses relative paths)\n",
        "    # Use exec to run in the same process so we can see output in real-time\n",
        "    try:\n",
        "        print(\"üìñ Reading training script...\")\n",
        "        # Read and execute the script\n",
        "        with open(script_path, 'r', encoding='utf-8') as f:\n",
        "            script_code = f.read()\n",
        "        print(\"‚úÖ Script loaded. Starting execution...\\n\")\n",
        "        \n",
        "        # Execute in current namespace (we're already in project directory)\n",
        "        exec(script_code, {'__name__': '__main__', '__file__': abs_script_path})\n",
        "        \n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(f\"‚úÖ TRAINING COMPLETED\")\n",
        "        print(f\"üìÖ End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        print(\"=\" * 70)\n",
        "        \n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n\\n‚ö†Ô∏è Training interrupted by user\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error during training: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"‚ùå train_model.py not found!\")\n",
        "    print(f\"  Expected location: {abs_script_path}\")\n",
        "    print(\"  Please ensure the script is in the scripts/ directory\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Check Training Results\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Models directory relative to project directory\n",
        "models_dir = Path('models')\n",
        "\n",
        "if models_dir.exists():\n",
        "    print(\"üìÅ Training Output Files:\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"  Location: {os.path.abspath(models_dir)}\")\n",
        "    print()\n",
        "    \n",
        "    files = list(models_dir.glob('*'))\n",
        "    if files:\n",
        "        for f in sorted(files):\n",
        "            size_mb = f.stat().st_size / (1024 * 1024) if f.is_file() else 0\n",
        "            file_type = \"üìÑ\" if f.is_file() else \"üìÅ\"\n",
        "            print(f\"{file_type} {f.name} ({size_mb:.2f} MB)\" if f.is_file() else f\"{file_type} {f.name}/\")\n",
        "    else:\n",
        "        print(\"  No files found yet. Training may still be in progress.\")\n",
        "    \n",
        "    # Check for summary file\n",
        "    summary_file = models_dir / 'training_summary.txt'\n",
        "    if summary_file.exists():\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"TRAINING SUMMARY\")\n",
        "        print(\"=\" * 70)\n",
        "        with open(summary_file, 'r') as f:\n",
        "            print(f.read())\n",
        "else:\n",
        "    print(\"‚ö† Models directory not found\")\n",
        "    print(f\"  Expected location: {os.path.abspath(models_dir)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Download Results (Optional)\n",
        "# ============================================================================\n",
        "\n",
        "from google.colab import files\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "\n",
        "# Models directory relative to project directory\n",
        "models_dir = Path('models')\n",
        "\n",
        "if models_dir.exists() and any(models_dir.iterdir()):\n",
        "    print(\"üì• Download training results?\")\n",
        "    print(f\"  Models directory: {os.path.abspath(models_dir)}\")\n",
        "    print(\"  Uncomment the code below to download all model files as a zip\")\n",
        "    \n",
        "    # Uncomment to enable download\n",
        "    # zip_path = '/content/training_results.zip'\n",
        "    # with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
        "    #     for file in models_dir.rglob('*'):\n",
        "    #         if file.is_file():\n",
        "    #             # Use relative path from project directory\n",
        "    #             zipf.write(file, file.relative_to(os.getcwd()))\n",
        "    # \n",
        "    # files.download(zip_path)\n",
        "    # print(\"‚úÖ Download started!\")\n",
        "else:\n",
        "    print(\"‚ö† No results to download yet\")\n",
        "    print(f\"  Models directory: {os.path.abspath(models_dir)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes:\n",
        "\n",
        "1. **GPU Setup**: Make sure to enable GPU in Colab (Runtime ‚Üí Change runtime type ‚Üí GPU)\n",
        "\n",
        "2. **Project Structure**: After mounting Drive, the notebook changes to the project directory (`/content/drive/MyDrive/DSAN6600final`). All paths are relative to this directory.\n",
        "\n",
        "3. **Data Path**: Data should be in `data/hi-zh.txt/` relative to project root\n",
        "\n",
        "4. **Sample Size**: Adjust `SAMPLE_SIZE` in the first cell to control how many pairs to use\n",
        "\n",
        "5. **Monitoring**: Training progress will be displayed in real-time in the notebook\n",
        "\n",
        "6. **Results**: All trained models and results will be saved to `models/` directory (relative to project root)\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ============================================================================\n",
        "# Prediction: Test Your Model\n",
        "# ============================================================================\n",
        "\n",
        "Use the cells below to predict alignment scores for Hindi-Chinese sentence pairs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Load Prediction Functions\n",
        "# ============================================================================\n",
        "\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Add scripts to path\n",
        "scripts_dir = os.path.join(os.getcwd(), 'scripts')\n",
        "if scripts_dir not in sys.path:\n",
        "    sys.path.insert(0, scripts_dir)\n",
        "\n",
        "# Import prediction functions\n",
        "try:\n",
        "    from predict_quality import predict_single, load_model_and_artifacts, extract_features\n",
        "    print(\"‚úÖ Prediction functions loaded successfully!\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Error loading prediction functions: {e}\")\n",
        "    print(\"  Make sure predict_quality.py is in the scripts/ directory\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Predict Alignment Score for a Sentence Pair\n",
        "# ============================================================================\n",
        "\n",
        "# Input your sentences here\n",
        "hindi_sentence = \"‡§Æ‡•à‡§Ç ‡§è‡§ï ‡§õ‡§æ‡§§‡•ç‡§∞ ‡§π‡•Ç‡§Å‡•§\"  # Change this to your Hindi sentence\n",
        "chinese_sentence = \"ÊàëÊòØ‰∏ÄÂêçÂ≠¶Áîü„ÄÇ\"  # Change this to your Chinese sentence\n",
        "\n",
        "# Check if model exists\n",
        "models_dir = Path('models')\n",
        "model_files = list(models_dir.glob('quality_estimation_*.h5')) + list(models_dir.glob('quality_estimation_*.pkl'))\n",
        "\n",
        "if model_files:\n",
        "    print(\"=\" * 70)\n",
        "    print(\"üîÆ PREDICTING ALIGNMENT SCORE\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\nüìù Input Sentences:\")\n",
        "    print(f\"  Hindi: {hindi_sentence}\")\n",
        "    print(f\"  Chinese: {chinese_sentence}\")\n",
        "    print(\"\\nüîÑ Computing prediction...\")\n",
        "    \n",
        "    try:\n",
        "        # Predict\n",
        "        score, confidence_interval = predict_single(hindi_sentence, chinese_sentence)\n",
        "        \n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"üìä PREDICTION RESULT\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"\\n‚úÖ Predicted Alignment Score: {score:.4f}\")\n",
        "        print(f\"üìà 95% Confidence Interval: [{confidence_interval[0]:.4f}, {confidence_interval[1]:.4f}]\")\n",
        "        print(f\"\\nüí° Interpretation:\")\n",
        "        print(f\"  ‚Ä¢ Higher scores indicate better alignment/similarity\")\n",
        "        print(f\"  ‚Ä¢ Typical range: ~1.06 to ~1.24 (based on CCMatrix scores)\")\n",
        "        if score > 1.15:\n",
        "            print(f\"  ‚Ä¢ This pair shows {'strong' if score > 1.20 else 'good'} alignment\")\n",
        "        elif score > 1.10:\n",
        "            print(f\"  ‚Ä¢ This pair shows moderate alignment\")\n",
        "        else:\n",
        "            print(f\"  ‚Ä¢ This pair shows lower alignment\")\n",
        "        print(\"=\" * 70)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error during prediction: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trained model found!\")\n",
        "    print(f\"  Expected location: {os.path.abspath(models_dir)}\")\n",
        "    print(\"  Please run the training cells first to train a model.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Batch Prediction (Multiple Pairs)\n",
        "# ============================================================================\n",
        "\n",
        "# Example: Predict scores for multiple sentence pairs\n",
        "sentence_pairs = [\n",
        "    (\"‡§Æ‡•à‡§Ç ‡§è‡§ï ‡§õ‡§æ‡§§‡•ç‡§∞ ‡§π‡•Ç‡§Å‡•§\", \"ÊàëÊòØ‰∏ÄÂêçÂ≠¶Áîü„ÄÇ\"),\n",
        "    (\"‡§Ø‡§π ‡§è‡§ï ‡§Ö‡§ö‡•ç‡§õ‡•Ä ‡§ï‡§ø‡§§‡§æ‡§¨ ‡§π‡•à‡•§\", \"ËøôÊòØ‰∏ÄÊú¨Â•Ω‰π¶„ÄÇ\"),\n",
        "    # Add more pairs here\n",
        "]\n",
        "\n",
        "if model_files:\n",
        "    print(\"=\" * 70)\n",
        "    print(\"üîÆ BATCH PREDICTION\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\nüìù Predicting scores for {len(sentence_pairs)} sentence pairs...\")\n",
        "    \n",
        "    try:\n",
        "        from predict_quality import predict_batch\n",
        "        \n",
        "        results = predict_batch(sentence_pairs)\n",
        "        \n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"üìä BATCH PREDICTION RESULTS\")\n",
        "        print(\"=\" * 70)\n",
        "        \n",
        "        for i, result in enumerate(results, 1):\n",
        "            print(f\"\\n{i}. Pair {i}:\")\n",
        "            print(f\"   Hindi: {result['hindi']}\")\n",
        "            print(f\"   Chinese: {result['chinese']}\")\n",
        "            print(f\"   üìä Score: {result['predicted_score']:.4f}\")\n",
        "            print(f\"   üìà CI: [{result['confidence_lower']:.4f}, {result['confidence_upper']:.4f}]\")\n",
        "        \n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        \n",
        "        # Create a summary DataFrame\n",
        "        import pandas as pd\n",
        "        df_results = pd.DataFrame(results)\n",
        "        print(\"\\nüìã Summary Table:\")\n",
        "        print(df_results[['predicted_score', 'confidence_lower', 'confidence_upper']].describe())\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error during batch prediction: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trained model found! Please run training first.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
