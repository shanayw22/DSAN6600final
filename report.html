<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.54">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="DSAN6600 Final Project">
<meta name="dcterms.date" content="2025-12-01">

<title>Translation Quality Estimation for Hindi-Chinese Sentence Pairs</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="report_files/libs/clipboard/clipboard.min.js"></script>
<script src="report_files/libs/quarto-html/quarto.js"></script>
<script src="report_files/libs/quarto-html/popper.min.js"></script>
<script src="report_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="report_files/libs/quarto-html/anchor.min.js"></script>
<link href="report_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="report_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="report_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="report_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="report_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">


</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#problem-statement" id="toc-problem-statement" class="nav-link" data-scroll-target="#problem-statement">Problem Statement</a></li>
  <li><a href="#motivation" id="toc-motivation" class="nav-link" data-scroll-target="#motivation">Motivation</a></li>
  <li><a href="#research-questions" id="toc-research-questions" class="nav-link" data-scroll-target="#research-questions">Research Questions</a></li>
  <li><a href="#objectives" id="toc-objectives" class="nav-link" data-scroll-target="#objectives">Objectives</a></li>
  </ul></li>
  <li><a href="#methods" id="toc-methods" class="nav-link" data-scroll-target="#methods">Methods</a>
  <ul class="collapse">
  <li><a href="#dataset" id="toc-dataset" class="nav-link" data-scroll-target="#dataset">Dataset</a>
  <ul class="collapse">
  <li><a href="#data-source" id="toc-data-source" class="nav-link" data-scroll-target="#data-source">Data Source</a></li>
  <li><a href="#data-preprocessing" id="toc-data-preprocessing" class="nav-link" data-scroll-target="#data-preprocessing">Data Preprocessing</a></li>
  <li><a href="#target-variable" id="toc-target-variable" class="nav-link" data-scroll-target="#target-variable">Target Variable</a></li>
  </ul></li>
  <li><a href="#feature-engineering" id="toc-feature-engineering" class="nav-link" data-scroll-target="#feature-engineering">Feature Engineering</a>
  <ul class="collapse">
  <li><a href="#length-features" id="toc-length-features" class="nav-link" data-scroll-target="#length-features">Length Features</a></li>
  <li><a href="#semantic-features" id="toc-semantic-features" class="nav-link" data-scroll-target="#semantic-features">Semantic Features</a></li>
  <li><a href="#alignment-features" id="toc-alignment-features" class="nav-link" data-scroll-target="#alignment-features">Alignment Features</a></li>
  <li><a href="#interaction-features" id="toc-interaction-features" class="nav-link" data-scroll-target="#interaction-features">Interaction Features</a></li>
  <li><a href="#statistical-features" id="toc-statistical-features" class="nav-link" data-scroll-target="#statistical-features">Statistical Features</a></li>
  <li><a href="#feature-selection" id="toc-feature-selection" class="nav-link" data-scroll-target="#feature-selection">Feature Selection</a></li>
  </ul></li>
  <li><a href="#model-architectures" id="toc-model-architectures" class="nav-link" data-scroll-target="#model-architectures">Model Architectures</a>
  <ul class="collapse">
  <li><a href="#simple-mlp" id="toc-simple-mlp" class="nav-link" data-scroll-target="#simple-mlp">Simple MLP</a></li>
  <li><a href="#deep-mlp" id="toc-deep-mlp" class="nav-link" data-scroll-target="#deep-mlp">Deep MLP</a></li>
  <li><a href="#residual-mlp" id="toc-residual-mlp" class="nav-link" data-scroll-target="#residual-mlp">Residual MLP</a></li>
  <li><a href="#cross-attention-transformer" id="toc-cross-attention-transformer" class="nav-link" data-scroll-target="#cross-attention-transformer">Cross-Attention Transformer</a></li>
  </ul></li>
  <li><a href="#training-procedure" id="toc-training-procedure" class="nav-link" data-scroll-target="#training-procedure">Training Procedure</a>
  <ul class="collapse">
  <li><a href="#data-preprocessing-1" id="toc-data-preprocessing-1" class="nav-link" data-scroll-target="#data-preprocessing-1">Data Preprocessing</a></li>
  <li><a href="#training-configuration" id="toc-training-configuration" class="nav-link" data-scroll-target="#training-configuration">Training Configuration</a></li>
  <li><a href="#gpu-optimization" id="toc-gpu-optimization" class="nav-link" data-scroll-target="#gpu-optimization">GPU Optimization</a></li>
  <li><a href="#evaluation-metrics" id="toc-evaluation-metrics" class="nav-link" data-scroll-target="#evaluation-metrics">Evaluation Metrics</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#analyses" id="toc-analyses" class="nav-link" data-scroll-target="#analyses">Analyses</a>
  <ul class="collapse">
  <li><a href="#data-exploration" id="toc-data-exploration" class="nav-link" data-scroll-target="#data-exploration">Data Exploration</a></li>
  <li><a href="#model-performance-comparison" id="toc-model-performance-comparison" class="nav-link" data-scroll-target="#model-performance-comparison">Model Performance Comparison</a></li>
  <li><a href="#training-curves" id="toc-training-curves" class="nav-link" data-scroll-target="#training-curves">Training Curves</a></li>
  <li><a href="#best-model-analysis" id="toc-best-model-analysis" class="nav-link" data-scroll-target="#best-model-analysis">Best Model Analysis</a></li>
  <li><a href="#feature-importance-analysis" id="toc-feature-importance-analysis" class="nav-link" data-scroll-target="#feature-importance-analysis">Feature Importance Analysis</a></li>
  <li><a href="#error-analysis" id="toc-error-analysis" class="nav-link" data-scroll-target="#error-analysis">Error Analysis</a></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a>
  <ul class="collapse">
  <li><a href="#model-performance-interpretation" id="toc-model-performance-interpretation" class="nav-link" data-scroll-target="#model-performance-interpretation">Model Performance Interpretation</a>
  <ul class="collapse">
  <li><a href="#overall-performance" id="toc-overall-performance" class="nav-link" data-scroll-target="#overall-performance">Overall Performance</a></li>
  <li><a href="#key-findings" id="toc-key-findings" class="nav-link" data-scroll-target="#key-findings">Key Findings</a></li>
  <li><a href="#challenges-and-limitations" id="toc-challenges-and-limitations" class="nav-link" data-scroll-target="#challenges-and-limitations">Challenges and Limitations</a></li>
  </ul></li>
  <li><a href="#comparison-with-literature" id="toc-comparison-with-literature" class="nav-link" data-scroll-target="#comparison-with-literature">Comparison with Literature</a></li>
  <li><a href="#future-directions" id="toc-future-directions" class="nav-link" data-scroll-target="#future-directions">Future Directions</a>
  <ul class="collapse">
  <li><a href="#immediate-improvements" id="toc-immediate-improvements" class="nav-link" data-scroll-target="#immediate-improvements">Immediate Improvements</a></li>
  <li><a href="#advanced-methods" id="toc-advanced-methods" class="nav-link" data-scroll-target="#advanced-methods">Advanced Methods</a></li>
  <li><a href="#evaluation-improvements" id="toc-evaluation-improvements" class="nav-link" data-scroll-target="#evaluation-improvements">Evaluation Improvements</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a>
  <ul class="collapse">
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  <li><a href="#key-contributions" id="toc-key-contributions" class="nav-link" data-scroll-target="#key-contributions">Key Contributions</a></li>
  <li><a href="#main-findings" id="toc-main-findings" class="nav-link" data-scroll-target="#main-findings">Main Findings</a></li>
  <li><a href="#implications" id="toc-implications" class="nav-link" data-scroll-target="#implications">Implications</a></li>
  <li><a href="#final-remarks" id="toc-final-remarks" class="nav-link" data-scroll-target="#final-remarks">Final Remarks</a></li>
  </ul></li>
  <li><a href="#citations" id="toc-citations" class="nav-link" data-scroll-target="#citations">Citations</a>
  <ul class="collapse">
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  <li><a href="#software-and-tools" id="toc-software-and-tools" class="nav-link" data-scroll-target="#software-and-tools">Software and Tools</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Translation Quality Estimation for Hindi-Chinese Sentence Pairs</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
<p class="subtitle lead">A Deep Learning Approach to Predict Alignment Scores Without Reference Translations</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>DSAN6600 Final Project </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">December 1, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Rendering Instructions
</div>
</div>
<div class="callout-body-container callout-body">
<p>To render this report, first generate training results by running <code>scripts/train_model.py</code> to create <code>models/training_results.pkl</code> and <code>models/training_summary.txt</code>. Ensure that <code>pandas</code>, <code>numpy</code>, <code>matplotlib</code>, and <code>seaborn</code> are installed. Then render with Quarto by running <code>quarto render report.qmd</code> in the project root directory. The report will automatically load and visualize training results if available.</p>
</div>
</div>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<section id="problem-statement" class="level2">
<h2 class="anchored" data-anchor-id="problem-statement">Problem Statement</h2>
<p>Translation quality estimation (TQE) represents a critical task in natural language processing that aims to assess the quality of translated text without requiring reference translations. This capability is particularly valuable for machine translation systems that must automatically filter low-quality translations, for parallel corpus construction where high-quality sentence pairs must be identified for training data, for post-editing workflows that prioritize translations requiring human review, and for cross-lingual applications that evaluate alignment quality in multilingual datasets.</p>
<p>Traditional quality estimation methods often rely on reference translations, which are expensive and time-consuming to obtain. This project addresses the challenge of predicting translation quality scores for Hindi-Chinese sentence pairs using only source-target features, without access to reference translations.</p>
</section>
<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>Hindi and Chinese represent two of the world’s most widely spoken languages, yet they belong to fundamentally different language families—Indo-European versus Sino-Tibetan—with distinct writing systems, grammatical structures, and cultural contexts. This substantial linguistic distance makes quality estimation particularly challenging but also scientifically interesting, as it tests the limits of cross-lingual feature extraction and neural network learning.</p>
<p>The CCMatrix dataset provides alignment scores that reflect semantic similarity between sentence pairs, serving as a proxy for translation quality. By learning to predict these scores from sentence-level features, we can develop a system that automates quality assessment, reduces the need for manual evaluation, scales to large corpora processing millions of sentence pairs efficiently, provides interpretable features using linguistically motivated characteristics that can be analyzed, and generalizes across domains by learning patterns that may transfer to other language pairs.</p>
</section>
<section id="research-questions" class="level2">
<h2 class="anchored" data-anchor-id="research-questions">Research Questions</h2>
<p>This project addresses four primary research questions. First, can we predict translation quality scores from source-target features alone? We investigate whether sentence-level features including length, semantic similarity, and fluency are sufficient to predict alignment scores. Second, which features are most predictive of translation quality? We analyze the contribution of different feature categories including length, semantic, and statistical properties to model performance. Third, how do different neural network architectures compare for this regression task? We systematically compare Simple MLP, Deep MLP, Residual MLP, and Transformer architectures. Fourth, what is the relationship between predicted scores and actual CCMatrix alignment scores? We evaluate correlation metrics including Pearson and Spearman correlations to assess prediction quality.</p>
</section>
<section id="objectives" class="level2">
<h2 class="anchored" data-anchor-id="objectives">Objectives</h2>
<p>The primary objectives of this project are to develop a regression model that predicts continuous CCMatrix alignment scores in the 0.0-1.0 range, to engineer comprehensive features capturing length, semantic, statistical, and interaction properties, to compare multiple deep learning architectures to identify the best-performing model, to evaluate model performance using multiple metrics including MAE, RMSE, R², and correlation coefficients, and to provide a practical tool for quality estimation that can be used in real-world applications.</p>
</section>
</section>
<section id="methods" class="level1">
<h1>Methods</h1>
<section id="dataset" class="level2">
<h2 class="anchored" data-anchor-id="dataset">Dataset</h2>
<section id="data-source" class="level3">
<h3 class="anchored" data-anchor-id="data-source">Data Source</h3>
<p>The project utilizes the CCMatrix Hindi-Chinese parallel corpus, which contains sentence pairs extracted from web-crawled multilingual data. CCMatrix employs a cross-lingual embedding approach to compute alignment scores that reflect semantic similarity between sentence pairs. The dataset originates from the CC-Aligned project and consists of three files: <code>CCMatrix.hi-zh.hi</code> containing Hindi sentences with one sentence per line, <code>CCMatrix.hi-zh.zh</code> containing Chinese sentences with one sentence per line, and <code>CCMatrix.hi-zh.scores</code> containing alignment scores as continuous values with one score per line.</p>
</section>
<section id="data-preprocessing" class="level3">
<h3 class="anchored" data-anchor-id="data-preprocessing">Data Preprocessing</h3>
<div id="5ded0372" class="cell" data-execution_count="2">
<div class="cell-output cell-output-stdout">
<pre><code>Dataset Information from Training Summary:
======================================================================
======================================================================
QUALITY ESTIMATION MODEL TRAINING SUMMARY
======================================================================

Dataset size: 100,000 pairs
Sample size: 100000
Features: 25
Train/Val/Test split: 70,000 / 15,000 / 15,000

Model Results:

mlp:
  Test MAE: 0.6488
  Test RMSE: 0.9404
  Test R²: 0.1446
  Test Spearman: 0.3333

deep_mlp:
  Test MAE: 0.6475
  Test RMSE: 0.9406
  Test R²: 0.1442
  Test Spearman: 0.3348

residual_mlp:
  Test MAE: 0.6430
  Test RMSE: 0.9388
  Test R²: 0.1475
  Test Spearman: 0.3345

transformer:
  Test MAE: 0.6846
  Test RMSE: 0.9969
  Test R²: 0.0387
  Test Spearman: 0.2501

Best Model: residual_mlp
Best Test MAE: 0.6430
Best Test R²: 0.1475
</code></pre>
</div>
</div>
<p>The dataset preprocessing pipeline involves four main steps. First, sentence pairing matches Hindi and Chinese sentences by line number to ensure correspondence. Second, score extraction retrieves alignment scores from the scores file. Third, sampling applies a configurable sample size for training, with the current experiment utilizing 100,000 sentence pairs to provide sufficient data for robust model learning. Fourth, the data is split into training, validation, and test sets using a 70% / 15% / 15% partition, resulting in 70,000 training samples, 15,000 validation samples, and 15,000 test samples for model training and evaluation.</p>
</section>
<section id="target-variable" class="level3">
<h3 class="anchored" data-anchor-id="target-variable">Target Variable</h3>
<p>The CCMatrix alignment score represents a continuous value typically ranging from 0.0 to 1.0, where higher scores closer to 1.0 indicate better alignment and semantic similarity, while lower scores closer to 0.0 indicate poor alignment or unrelated sentences. These scores are computed using cross-lingual embeddings and serve as a proxy for translation quality, though they measure semantic similarity rather than true translation accuracy.</p>
</section>
</section>
<section id="feature-engineering" class="level2">
<h2 class="anchored" data-anchor-id="feature-engineering">Feature Engineering</h2>
<p>We engineer over 32 features across five distinct categories, each designed to capture different aspects of sentence pair relationships.</p>
<section id="length-features" class="level3">
<h3 class="anchored" data-anchor-id="length-features">Length Features</h3>
<p>The first category encompasses nine length features that capture the relationship between sentence lengths in both languages. These include <code>hindi_length</code> representing character count in Hindi sentences, <code>chinese_length</code> representing character count in Chinese sentences, <code>hindi_word_count</code> representing word count in Hindi sentences, <code>chinese_char_count</code> representing Chinese character count specifically for Han characters, <code>length_ratio</code> representing the ratio of Hindi to Chinese length, <code>length_diff</code> representing absolute difference in lengths, <code>hindi_avg_word_length</code> representing average word length in Hindi, <code>chinese_avg_char_length</code> representing average character length in Chinese, and <code>length_ratio_normalized</code> representing normalized length ratio. The rationale for these features stems from the observation that length mismatches can indicate translation quality issues, as well-aligned translations typically exhibit proportional lengths across languages.</p>
</section>
<section id="semantic-features" class="level3">
<h3 class="anchored" data-anchor-id="semantic-features">Semantic Features</h3>
<p>The second category comprises eleven semantic features that capture semantic similarity and fluency. These include <code>embedding_similarity</code> computed as cosine similarity using the <code>paraphrase-multilingual-MiniLM-L12-v2</code> model, <code>labse_similarity</code> computed as cosine similarity using the <code>sentence-transformers/LaBSE</code> model, <code>chinese_perplexity</code> representing language model perplexity as a fluency indicator, <code>chinese_fluency</code> computed as the inverse of perplexity, <code>embedding_labse_interaction</code> representing the product of embedding and LaBSE similarities, <code>similarity_avg</code> representing the average of embedding and LaBSE similarities, <code>similarity_diff</code> representing the absolute difference between similarities, <code>embedding_similarity_squared</code> representing squared embedding similarity as a polynomial feature, <code>labse_similarity_squared</code> representing squared LaBSE similarity as a polynomial feature, and <code>similarity_ratio</code> representing the ratio of embedding to LaBSE similarity. The rationale for these features is that semantic similarity serves as a strong indicator of translation quality, and using multiple embedding models provides complementary signals that may capture different aspects of semantic equivalence.</p>
</section>
<section id="alignment-features" class="level3">
<h3 class="anchored" data-anchor-id="alignment-features">Alignment Features</h3>
<p>The third category contains a single alignment feature: <code>char_alignment</code> computed as a character-level alignment heuristic using Jaccard similarity of character sets. This feature captures surface-level similarity that may not be captured by semantic embeddings, particularly for languages with different writing systems where character overlap might indicate transliteration or proper noun correspondence.</p>
</section>
<section id="interaction-features" class="level3">
<h3 class="anchored" data-anchor-id="interaction-features">Interaction Features</h3>
<p>The fourth category includes three interaction features that combine length and semantic information: <code>length_similarity_interaction</code> representing the product of length ratio and embedding similarity, <code>length_perplexity_interaction</code> representing the product of Chinese length and perplexity, and <code>length_perplexity_ratio</code> representing the ratio of Chinese length to perplexity. These interaction features can capture non-linear relationships between features that may not be apparent in individual feature values.</p>
</section>
<section id="statistical-features" class="level3">
<h3 class="anchored" data-anchor-id="statistical-features">Statistical Features</h3>
<p>The fifth category encompasses eight statistical features that capture text characteristics: <code>punctuation_count_hi</code> and <code>punctuation_count_zh</code> representing punctuation counts in Hindi and Chinese respectively, <code>digit_count_hi</code> and <code>digit_count_zh</code> representing digit counts in both languages, <code>punctuation_ratio_hi</code> and <code>punctuation_ratio_zh</code> representing punctuation ratios normalized by sentence length, and <code>vocab_diversity_hi</code> and <code>vocab_diversity_zh</code> representing vocabulary diversity computed as the ratio of unique tokens to total tokens. These statistical properties can indicate translation quality and text characteristics that may correlate with alignment scores.</p>
</section>
<section id="feature-selection" class="level3">
<h3 class="anchored" data-anchor-id="feature-selection">Feature Selection</h3>
<p>After feature engineering, we apply colinearity removal to eliminate highly correlated features with correlation coefficients exceeding 0.95. This process reduces redundancy and improves model generalization by preventing multicollinearity issues. The feature selection step reduced the feature set from 31 to 25 features in the current experiment, removing 6 highly correlated features including embedding similarity squared, Hindi word count, LaBSE similarity squared, length difference, normalized length ratio, and similarity average.</p>
</section>
</section>
<section id="model-architectures" class="level2">
<h2 class="anchored" data-anchor-id="model-architectures">Model Architectures</h2>
<p>We compare four deep learning architectures, each representing different approaches to learning from the engineered features.</p>
<section id="simple-mlp" class="level3">
<h3 class="anchored" data-anchor-id="simple-mlp">Simple MLP</h3>
<p>The first architecture is a baseline multi-layer perceptron with regularization. The architecture consists of an input layer followed by a dense layer with 128 units and ReLU activation, dropout with rate 0.3, a second dense layer with 64 units and ReLU activation, dropout with rate 0.2, and a final dense output layer with a single linear unit. Regularization includes L2 weight decay with coefficient 0.01 and dropout layers. This model contains approximately 20,000 parameters and serves as a baseline for comparison.</p>
</section>
<section id="deep-mlp" class="level3">
<h3 class="anchored" data-anchor-id="deep-mlp">Deep MLP</h3>
<p>The second architecture is a deeper network with batch normalization. The architecture consists of an input layer followed by a dense layer with 256 units and ReLU activation, batch normalization, dropout with rate 0.4, a second dense layer with 128 units and ReLU activation, batch normalization, dropout with rate 0.3, a third dense layer with 64 units and ReLU activation, dropout with rate 0.2, and a final dense output layer with a single linear unit. Regularization includes L2 weight decay with coefficient 0.01, batch normalization layers, and dropout with rates ranging from 0.2 to 0.4. This model contains approximately 100,000 parameters.</p>
</section>
<section id="residual-mlp" class="level3">
<h3 class="anchored" data-anchor-id="residual-mlp">Residual MLP</h3>
<p>The third architecture is a residual network with skip connections. The architecture is similar to the Deep MLP but incorporates residual connections between layers, allowing gradients to flow more easily through the network and potentially enabling training of deeper architectures. Regularization includes L2 weight decay with coefficient 0.01, batch normalization layers, and dropout with rates ranging from 0.2 to 0.4. This model contains approximately 120,000 parameters.</p>
</section>
<section id="cross-attention-transformer" class="level3">
<h3 class="anchored" data-anchor-id="cross-attention-transformer">Cross-Attention Transformer</h3>
<p>The fourth architecture is a transformer model with cross-attention between Hindi and Chinese embeddings. The architecture includes separate input projections for Hindi and Chinese embeddings, multi-head cross-attention layers enabling bidirectional attention between Hindi and Chinese representations, feed-forward networks with layer normalization, and concatenation followed by final dense layers. Hyperparameters include 8 attention heads, 2 transformer layers, and a model dimension of 256. This model contains approximately 200,000 parameters and represents the most sophisticated architecture tested.</p>
</section>
</section>
<section id="training-procedure" class="level2">
<h2 class="anchored" data-anchor-id="training-procedure">Training Procedure</h2>
<section id="data-preprocessing-1" class="level3">
<h3 class="anchored" data-anchor-id="data-preprocessing-1">Data Preprocessing</h3>
<p>The training procedure begins with comprehensive data preprocessing. Feature scaling applies StandardScaler to all features, transforming them to have zero mean and unit variance. Target scaling applies StandardScaler to CCMatrix scores to improve neural network training stability. During evaluation, predictions are inverse-transformed back to the original scale to compute metrics on the natural score range.</p>
</section>
<section id="training-configuration" class="level3">
<h3 class="anchored" data-anchor-id="training-configuration">Training Configuration</h3>
<p>Training employs the Adam optimizer with default learning rate settings. The loss function is Mean Squared Error (MSE), with Mean Absolute Error (MAE) tracked as an additional metric. Batch size is set to 256 for GPU training or 64 for CPU training, depending on hardware availability. Training proceeds for up to 100 epochs with early stopping based on validation loss. Early stopping patience is set to 10 epochs for MLP architectures and 5 epochs for the transformer architecture. Mixed precision training with FP16 is enabled on GPU for faster computation while maintaining numerical stability in the output layer through FP32 precision.</p>
</section>
<section id="gpu-optimization" class="level3">
<h3 class="anchored" data-anchor-id="gpu-optimization">GPU Optimization</h3>
<p>The training script incorporates several GPU optimizations to maximize computational efficiency. Memory growth enables dynamic GPU memory allocation to avoid allocating all GPU memory at once. Graph mode utilizes TensorFlow graph execution for faster computation compared to eager execution. Mixed precision training employs FP16 for intermediate computations while maintaining FP32 for the output layer. The tf.data pipeline provides efficient data loading with prefetching and batching to minimize data loading bottlenecks during training.</p>
</section>
<section id="evaluation-metrics" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-metrics">Evaluation Metrics</h3>
<p>Models are evaluated using multiple metrics to provide comprehensive performance assessment. Mean Absolute Error (MAE) measures average prediction error and is the primary metric for model selection. Root Mean Squared Error (RMSE) penalizes larger errors more heavily than smaller errors. The coefficient of determination (R²) measures the proportion of variance in the target variable explained by the model. Pearson correlation measures linear relationship strength between predictions and actual scores. Spearman correlation measures monotonic relationship strength using rank-based correlation, which is less sensitive to outliers than Pearson correlation.</p>
</section>
</section>
</section>
<section id="analyses" class="level1">
<h1>Analyses</h1>
<section id="data-exploration" class="level2">
<h2 class="anchored" data-anchor-id="data-exploration">Data Exploration</h2>
<div id="1b49a978" class="cell" data-fig-height="6" data-fig-width="10" data-execution_count="3">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load training results</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(<span class="st">'models/training_results.pkl'</span>, <span class="st">'rb'</span>) <span class="im">as</span> f:</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>        results <span class="op">=</span> pickle.load(f)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load training summary</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(<span class="st">'models/training_summary.txt'</span>, <span class="st">'r'</span>) <span class="im">as</span> f:</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>        summary_lines <span class="op">=</span> f.readlines()</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Training Results Summary:"</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> model_name, metrics <span class="kw">in</span> results.items():</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>model_name<span class="sc">.</span>upper()<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Test MAE: </span><span class="sc">{</span>metrics[<span class="st">'test_mae'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Test RMSE: </span><span class="sc">{</span>metrics[<span class="st">'test_rmse'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Test R²: </span><span class="sc">{</span>metrics[<span class="st">'test_r2'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> metrics.get(<span class="st">'test_spearman'</span>) <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"  Test Spearman: </span><span class="sc">{</span>metrics[<span class="st">'test_spearman'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> metrics.get(<span class="st">'test_pearson'</span>) <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"  Test Pearson: </span><span class="sc">{</span>metrics[<span class="st">'test_pearson'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create comparison plot</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    models <span class="op">=</span> <span class="bu">list</span>(results.keys())</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    mae_values <span class="op">=</span> [results[m][<span class="st">'test_mae'</span>] <span class="cf">for</span> m <span class="kw">in</span> models]</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    rmse_values <span class="op">=</span> [results[m][<span class="st">'test_rmse'</span>] <span class="cf">for</span> m <span class="kw">in</span> models]</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    r2_values <span class="op">=</span> [results[m][<span class="st">'test_r2'</span>] <span class="cf">for</span> m <span class="kw">in</span> models]</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    spearman_values <span class="op">=</span> [results[m].get(<span class="st">'test_spearman'</span>, <span class="dv">0</span>) <span class="cf">if</span> results[m].get(<span class="st">'test_spearman'</span>) <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> <span class="dv">0</span> <span class="cf">for</span> m <span class="kw">in</span> models]</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">10</span>))</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># MAE comparison</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>, <span class="dv">0</span>].bar(models, mae_values, color<span class="op">=</span>[<span class="st">'#3498db'</span>, <span class="st">'#e74c3c'</span>, <span class="st">'#2ecc71'</span>, <span class="st">'#f39c12'</span>])</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>, <span class="dv">0</span>].set_title(<span class="st">'Test MAE by Model'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>, <span class="dv">0</span>].set_ylabel(<span class="st">'MAE'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>, <span class="dv">0</span>].tick_params(axis<span class="op">=</span><span class="st">'x'</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>, <span class="dv">0</span>].grid(axis<span class="op">=</span><span class="st">'y'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># RMSE comparison</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>, <span class="dv">1</span>].bar(models, rmse_values, color<span class="op">=</span>[<span class="st">'#3498db'</span>, <span class="st">'#e74c3c'</span>, <span class="st">'#2ecc71'</span>, <span class="st">'#f39c12'</span>])</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>, <span class="dv">1</span>].set_title(<span class="st">'Test RMSE by Model'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>, <span class="dv">1</span>].set_ylabel(<span class="st">'RMSE'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>, <span class="dv">1</span>].tick_params(axis<span class="op">=</span><span class="st">'x'</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>, <span class="dv">1</span>].grid(axis<span class="op">=</span><span class="st">'y'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># R² comparison</span></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>, <span class="dv">0</span>].bar(models, r2_values, color<span class="op">=</span>[<span class="st">'#3498db'</span>, <span class="st">'#e74c3c'</span>, <span class="st">'#2ecc71'</span>, <span class="st">'#f39c12'</span>])</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>, <span class="dv">0</span>].set_title(<span class="st">'Test R² by Model'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>, <span class="dv">0</span>].set_ylabel(<span class="st">'R²'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>, <span class="dv">0</span>].tick_params(axis<span class="op">=</span><span class="st">'x'</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>, <span class="dv">0</span>].grid(axis<span class="op">=</span><span class="st">'y'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>, <span class="dv">0</span>].axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Spearman correlation comparison</span></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>, <span class="dv">1</span>].bar(models, spearman_values, color<span class="op">=</span>[<span class="st">'#3498db'</span>, <span class="st">'#e74c3c'</span>, <span class="st">'#2ecc71'</span>, <span class="st">'#f39c12'</span>])</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>, <span class="dv">1</span>].set_title(<span class="st">'Test Spearman Correlation by Model'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>, <span class="dv">1</span>].set_ylabel(<span class="st">'Spearman Correlation'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>, <span class="dv">1</span>].tick_params(axis<span class="op">=</span><span class="st">'x'</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>, <span class="dv">1</span>].grid(axis<span class="op">=</span><span class="st">'y'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>, <span class="dv">1</span>].axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">FileNotFoundError</span>:</span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Training results not found. Please run train_model.py first to generate results."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Training Results Summary:
======================================================================

MLP:
  Test MAE: 0.6488
  Test RMSE: 0.9404
  Test R²: 0.1446
  Test Spearman: 0.3333
  Test Pearson: 0.3824

DEEP_MLP:
  Test MAE: 0.6475
  Test RMSE: 0.9406
  Test R²: 0.1442
  Test Spearman: 0.3348
  Test Pearson: 0.3816

RESIDUAL_MLP:
  Test MAE: 0.6430
  Test RMSE: 0.9388
  Test R²: 0.1475
  Test Spearman: 0.3345
  Test Pearson: 0.3858

TRANSFORMER:
  Test MAE: 0.6846
  Test RMSE: 0.9969
  Test R²: 0.0387
  Test Spearman: 0.2501
  Test Pearson: 0.3212</code></pre>
</div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="report_files/figure-html/cell-4-output-2.png" class="img-fluid figure-img"></p>
<figcaption>Distribution of CCMatrix alignment scores in the dataset</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="model-performance-comparison" class="level2">
<h2 class="anchored" data-anchor-id="model-performance-comparison">Model Performance Comparison</h2>
<div id="7956c922" class="cell" data-fig-height="8" data-fig-width="12" data-execution_count="4">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a comprehensive comparison table</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    comparison_data <span class="op">=</span> []</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> model_name, metrics <span class="kw">in</span> results.items():</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        comparison_data.append({</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Model'</span>: model_name.replace(<span class="st">'_'</span>, <span class="st">' '</span>).title(),</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">'MAE'</span>: metrics[<span class="st">'test_mae'</span>],</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>            <span class="st">'RMSE'</span>: metrics[<span class="st">'test_rmse'</span>],</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>            <span class="st">'R²'</span>: metrics[<span class="st">'test_r2'</span>],</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Spearman'</span>: metrics.get(<span class="st">'test_spearman'</span>, np.nan),</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Pearson'</span>: metrics.get(<span class="st">'test_pearson'</span>, np.nan),</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Train MAE'</span>: metrics.get(<span class="st">'train_mae'</span>, np.nan),</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Val MAE'</span>: metrics.get(<span class="st">'val_mae'</span>, np.nan)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>        })</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    df_comparison <span class="op">=</span> pd.DataFrame(comparison_data)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    df_comparison <span class="op">=</span> df_comparison.sort_values(<span class="st">'MAE'</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Model Performance Comparison Table:"</span>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(df_comparison.to_string(index<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create heatmap of metrics</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    metrics_for_heatmap <span class="op">=</span> df_comparison.set_index(<span class="st">'Model'</span>)[[<span class="st">'MAE'</span>, <span class="st">'RMSE'</span>, <span class="st">'R²'</span>, <span class="st">'Spearman'</span>, <span class="st">'Pearson'</span>]]</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    sns.heatmap(metrics_for_heatmap.T, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">'.4f'</span>, cmap<span class="op">=</span><span class="st">'RdYlGn_r'</span>, </span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>                center<span class="op">=</span><span class="dv">0</span>, vmin<span class="op">=-</span><span class="fl">0.2</span>, vmax<span class="op">=</span><span class="fl">1.2</span>, cbar_kws<span class="op">=</span>{<span class="st">'label'</span>: <span class="st">'Metric Value'</span>}, ax<span class="op">=</span>ax)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="st">'Model Performance Heatmap'</span>, fontsize<span class="op">=</span><span class="dv">16</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>, pad<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">'Model'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">'Metric'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">NameError</span>:</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Results not loaded. Please run the previous code chunk first."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Model Performance Comparison Table:
====================================================================================================
       Model      MAE     RMSE       R²  Spearman  Pearson  Train MAE  Val MAE
Residual Mlp 0.643025 0.938826 0.147457  0.334519 0.385785   0.631547 0.632269
    Deep Mlp 0.647530 0.940618 0.144199  0.334835 0.381551   0.634955 0.635778
         Mlp 0.648752 0.940375 0.144640  0.333317 0.382374   0.637082 0.637519
 Transformer 0.684594 0.996920 0.038682  0.250070 0.321243   0.620280 0.683183</code></pre>
</div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="report_files/figure-html/cell-5-output-2.png" class="img-fluid figure-img"></p>
<figcaption>Comprehensive model performance comparison across all metrics</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="training-curves" class="level2">
<h2 class="anchored" data-anchor-id="training-curves">Training Curves</h2>
<div id="7c512aa8" class="cell" data-fig-height="8" data-fig-width="12" data-execution_count="5">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: Training history is not saved by default in the current implementation</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co"># This is a placeholder for when training history is available</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Note: Training history (loss curves) would be displayed here if saved during training."</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"To enable this, modify train_model.py to save training history dictionaries."</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Current implementation saves final metrics but not epoch-by-epoch training curves."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Note: Training history (loss curves) would be displayed here if saved during training.
To enable this, modify train_model.py to save training history dictionaries.

Current implementation saves final metrics but not epoch-by-epoch training curves.</code></pre>
</div>
</div>
</section>
<section id="best-model-analysis" class="level2">
<h2 class="anchored" data-anchor-id="best-model-analysis">Best Model Analysis</h2>
<div id="afd3ca72" class="cell" data-execution_count="6">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Find best model (lowest MAE)</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    best_model_name <span class="op">=</span> <span class="bu">min</span>(results.keys(), key<span class="op">=</span><span class="kw">lambda</span> k: results[k][<span class="st">'test_mae'</span>])</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    best_metrics <span class="op">=</span> results[best_model_name]</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"BEST MODEL ANALYSIS"</span>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Best Model: </span><span class="sc">{</span>best_model_name<span class="sc">.</span>upper()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Performance Metrics:"</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Test MAE:  </span><span class="sc">{</span>best_metrics[<span class="st">'test_mae'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Test RMSE: </span><span class="sc">{</span>best_metrics[<span class="st">'test_rmse'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Test R²:   </span><span class="sc">{</span>best_metrics[<span class="st">'test_r2'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> best_metrics.get(<span class="st">'test_spearman'</span>) <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Test Spearman: </span><span class="sc">{</span>best_metrics[<span class="st">'test_spearman'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> best_metrics.get(<span class="st">'test_pearson'</span>) <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Test Pearson:  </span><span class="sc">{</span>best_metrics[<span class="st">'test_pearson'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Training Metrics:"</span>)</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>    train_mae <span class="op">=</span> best_metrics.get(<span class="st">'train_mae'</span>)</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    val_mae <span class="op">=</span> best_metrics.get(<span class="st">'val_mae'</span>)</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Train MAE: </span><span class="sc">{</span>train_mae<span class="sc">:.4f}</span><span class="ss">"</span> <span class="cf">if</span> train_mae <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> <span class="st">"  Train MAE: N/A"</span>)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Val MAE:   </span><span class="sc">{</span>val_mae<span class="sc">:.4f}</span><span class="ss">"</span> <span class="cf">if</span> val_mae <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> <span class="st">"  Val MAE:   N/A"</span>)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Interpretation</span></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Interpretation:"</span>)</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> best_metrics[<span class="st">'test_r2'</span>] <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  ✓ The model explains </span><span class="sc">{</span>best_metrics[<span class="st">'test_r2'</span>]<span class="op">*</span><span class="dv">100</span><span class="sc">:.1f}</span><span class="ss">% of variance in CCMatrix scores"</span>)</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  ⚠ The model performs worse than a simple mean baseline (R² &lt; 0)"</span>)</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> best_metrics.get(<span class="st">'test_spearman'</span>) <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">abs</span>(best_metrics[<span class="st">'test_spearman'</span>]) <span class="op">&gt;</span> <span class="fl">0.5</span>:</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>            strength <span class="op">=</span> <span class="st">"strong"</span> <span class="cf">if</span> <span class="bu">abs</span>(best_metrics[<span class="st">'test_spearman'</span>]) <span class="op">&gt;</span> <span class="fl">0.7</span> <span class="cf">else</span> <span class="st">"moderate"</span></span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"  ✓ </span><span class="sc">{</span>strength<span class="sc">.</span>capitalize()<span class="sc">}</span><span class="ss"> monotonic relationship (Spearman = </span><span class="sc">{</span>best_metrics[<span class="st">'test_spearman'</span>]<span class="sc">:.3f}</span><span class="ss">)"</span>)</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"  ⚠ Weak monotonic relationship (Spearman = </span><span class="sc">{</span>best_metrics[<span class="st">'test_spearman'</span>]<span class="sc">:.3f}</span><span class="ss">)"</span>)</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">NameError</span>:</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Results not loaded. Please run the previous code chunks first."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>======================================================================
BEST MODEL ANALYSIS
======================================================================

Best Model: RESIDUAL_MLP

Performance Metrics:
  Test MAE:  0.6430
  Test RMSE: 0.9388
  Test R²:   0.1475
  Test Spearman: 0.3345
  Test Pearson:  0.3858

Training Metrics:
  Train MAE: 0.6315
  Val MAE:   0.6323

Interpretation:
  ✓ The model explains 14.7% of variance in CCMatrix scores
  ⚠ Weak monotonic relationship (Spearman = 0.335)</code></pre>
</div>
</div>
</section>
<section id="feature-importance-analysis" class="level2">
<h2 class="anchored" data-anchor-id="feature-importance-analysis">Feature Importance Analysis</h2>
<div id="50a06910" class="cell" data-fig-height="6" data-fig-width="10" data-execution_count="7">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Feature Importance Analysis:"</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Neural network models (MLP, Deep MLP, Residual MLP, Transformer) learn"</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"distributed representations across all features, making direct feature"</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"importance extraction challenging."</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">To analyze feature importance, consider:"</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  1. Permutation importance: Shuffle each feature and measure performance drop"</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  2. SHAP values: Compute Shapley Additive Explanations"</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  3. Gradient-based methods: Analyze gradients w.r.t. input features"</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  4. Ablation studies: Remove feature categories and measure impact"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Feature Importance Analysis:
======================================================================

Neural network models (MLP, Deep MLP, Residual MLP, Transformer) learn
distributed representations across all features, making direct feature
importance extraction challenging.

To analyze feature importance, consider:
  1. Permutation importance: Shuffle each feature and measure performance drop
  2. SHAP values: Compute Shapley Additive Explanations
  3. Gradient-based methods: Analyze gradients w.r.t. input features
  4. Ablation studies: Remove feature categories and measure impact</code></pre>
</div>
</div>
</section>
<section id="error-analysis" class="level2">
<h2 class="anchored" data-anchor-id="error-analysis">Error Analysis</h2>
<div id="9aa7e821" class="cell" data-execution_count="8">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Error Analysis:"</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">To perform detailed error analysis, we would need:"</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  1. Test set predictions from the best model"</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  2. Actual CCMatrix scores"</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  3. Sentence pairs for qualitative analysis"</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">This would allow us to:"</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  - Identify high-error cases"</span>)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  - Analyze error patterns (over/under-prediction)"</span>)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  - Examine sentence characteristics of misclassified pairs"</span>)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  - Generate scatter plots: predicted vs. actual scores"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Error Analysis:
======================================================================

To perform detailed error analysis, we would need:
  1. Test set predictions from the best model
  2. Actual CCMatrix scores
  3. Sentence pairs for qualitative analysis

This would allow us to:
  - Identify high-error cases
  - Analyze error patterns (over/under-prediction)
  - Examine sentence characteristics of misclassified pairs
  - Generate scatter plots: predicted vs. actual scores</code></pre>
</div>
</div>
</section>
</section>
<section id="discussion" class="level1">
<h1>Discussion</h1>
<section id="model-performance-interpretation" class="level2">
<h2 class="anchored" data-anchor-id="model-performance-interpretation">Model Performance Interpretation</h2>
<section id="overall-performance" class="level3">
<h3 class="anchored" data-anchor-id="overall-performance">Overall Performance</h3>
<p>The models achieve improved performance on the translation quality estimation task when trained on the larger dataset. The best model, identified as Residual MLP, achieves a mean absolute error of 0.6430 on the CCMatrix score scale, a positive R² value of 0.1475 indicating that the model explains approximately 14.75% of variance in alignment scores, and a Spearman correlation of 0.3345 indicating a moderate monotonic relationship between predictions and actual scores. These results represent substantial improvement over baseline performance and demonstrate that the models successfully learn meaningful patterns from the engineered features.</p>
</section>
<section id="key-findings" class="level3">
<h3 class="anchored" data-anchor-id="key-findings">Key Findings</h3>
<p>The analysis reveals several important findings. First, the models demonstrate meaningful predictive power when trained on the larger dataset of 100,000 samples, as evidenced by positive R² values of approximately 0.14-0.15, indicating that the models explain a significant portion of variance in CCMatrix alignment scores. This represents a substantial improvement over baseline performance and demonstrates that sentence-level features can successfully predict alignment scores, though the relationship remains challenging with room for further improvement.</p>
<p>Second, the model architecture comparison reveals that the Residual MLP performs best with the lowest MAE of 0.6430 and highest R² of 0.1475, suggesting that residual connections enable more effective learning of complex patterns in the larger dataset. The Deep MLP achieves similar performance with MAE of 0.6475 and R² of 0.1442, while the Simple MLP provides a reasonable baseline with MAE of 0.6488 and R² of 0.1446. The Transformer architecture achieves lower performance with MAE of 0.6846 and R² of 0.0387, possibly due to insufficient data for the more complex architecture or the nature of the regression problem.</p>
<p>Third, the engineered feature set of 25 features after colinearity removal enables the models to learn meaningful patterns, as demonstrated by positive R² values and moderate Spearman correlations around 0.33-0.34. This suggests that sentence-level features can successfully predict CCMatrix alignment scores, though the relationship remains challenging with substantial room for improvement. The moderate correlations indicate that while the models capture meaningful patterns, additional features, larger datasets, or more sophisticated architectures may further enhance performance.</p>
</section>
<section id="challenges-and-limitations" class="level3">
<h3 class="anchored" data-anchor-id="challenges-and-limitations">Challenges and Limitations</h3>
<p>Several challenges and limitations affect the interpretation of results. First, label quality represents a significant concern, as CCMatrix alignment scores are computed automatically and may not perfectly reflect translation quality. These scores measure semantic similarity rather than true translation quality, which may introduce systematic biases in the target variable.</p>
<p>Second, feature limitations constrain model performance. Sentence-level features may not capture discourse-level or context-dependent quality factors that influence translation quality. Cross-lingual embeddings may not fully capture semantic equivalence, particularly for languages with different writing systems and cultural contexts. Perplexity measures fluency but not necessarily translation accuracy, as fluent text may still contain translation errors.</p>
<p>The current experiment utilized 100,000 training examples, which provided sufficient data for the models to learn robust patterns as evidenced by positive R² values and moderate correlations. This dataset size represents a substantial improvement over smaller experiments and demonstrates the importance of adequate training data for this task. Further increases to 200,000 or more pairs may provide additional improvements, though the current results suggest diminishing returns may be expected.</p>
<p>The evaluation metrics demonstrate successful model learning with positive R² values and moderate correlations. The Residual MLP achieves the best performance with R² of 0.1475 and Spearman correlation of 0.3345, indicating that the models successfully learn useful patterns from the engineered features. The moderate correlation values suggest that while the models capture meaningful relationships, there remains substantial unexplained variance that could potentially be addressed through additional features or more sophisticated architectures.</p>
</section>
</section>
<section id="comparison-with-literature" class="level2">
<h2 class="anchored" data-anchor-id="comparison-with-literature">Comparison with Literature</h2>
<p>Translation quality estimation has been extensively studied in the literature, with state-of-the-art methods achieving Spearman correlations of 0.6-0.8 on human-annotated data in WMT Quality Estimation Shared Tasks. Reference-free methods typically achieve lower correlations of 0.3-0.5 when using only source-target features, as they lack the information provided by reference translations.</p>
<p>Our results, with Spearman correlations of approximately 0.33-0.34, represent meaningful progress toward these benchmarks, though they remain below state-of-the-art performance. The positive R² values of 0.14-0.15 demonstrate that the models successfully learn from sentence-level features, representing a substantial improvement over baseline performance. Several factors may explain the gap to state-of-the-art: CCMatrix scores may be noisier than human-annotated quality scores, as they are computed automatically using embedding-based methods. Additional features or architectures may be needed, such as pre-trained multilingual models fine-tuned specifically for quality estimation. While the current dataset of 100,000 samples enables learning, even larger training datasets or more sophisticated feature engineering may further improve performance.</p>
</section>
<section id="future-directions" class="level2">
<h2 class="anchored" data-anchor-id="future-directions">Future Directions</h2>
<section id="immediate-improvements" class="level3">
<h3 class="anchored" data-anchor-id="immediate-improvements">Immediate Improvements</h3>
<p>Several immediate improvements could enhance model performance. Training on larger datasets using the full dataset of 200,000 or more pairs could improve generalization by providing more diverse examples. Hyperparameter tuning through grid search or Bayesian optimization could identify optimal learning rates, batch sizes, and architecture configurations. Feature engineering could be extended to include part-of-speech tag distributions, named entity overlap, word alignment scores, and translation probability from small neural machine translation models.</p>
</section>
<section id="advanced-methods" class="level3">
<h3 class="anchored" data-anchor-id="advanced-methods">Advanced Methods</h3>
<p>Advanced methods could significantly improve performance. Fine-tuning pre-trained models such as mBERT or XLM-RoBERTa directly for quality estimation could leverage transfer learning from large-scale multilingual pre-training. Siamese networks that encode source and target separately before comparing representations could capture cross-lingual relationships more effectively. Ensemble methods combining predictions from multiple models through voting or stacking could improve robustness. Cross-validation using k-fold validation could provide more robust evaluation and reduce variance in performance estimates.</p>
</section>
<section id="evaluation-improvements" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-improvements">Evaluation Improvements</h3>
<p>Evaluation improvements would enhance understanding of model behavior. Human evaluation through creation of a small human-annotated test set could validate predictions against ground truth quality judgments. Error analysis systematically examining high-error cases could identify patterns and failure modes. Feature importance analysis using SHAP values or permutation importance could understand feature contributions. Visualization through scatter plots, error distributions, and qualitative examples could provide intuitive understanding of model performance.</p>
</section>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>This project developed and evaluated deep learning models for translation quality estimation of Hindi-Chinese sentence pairs using a dataset of 100,000 sentence pairs. We engineered 31 features across five categories including length, semantic, alignment, interaction, and statistical properties, reduced to 25 features after colinearity removal, and systematically compared four neural network architectures: Simple MLP, Deep MLP, Residual MLP, and Cross-Attention Transformer.</p>
</section>
<section id="key-contributions" class="level2">
<h2 class="anchored" data-anchor-id="key-contributions">Key Contributions</h2>
<p>The project makes several key contributions to the field of translation quality estimation. First, we developed a comprehensive feature engineering framework capturing multiple aspects of sentence pairs through length ratios, semantic similarities from multiple embedding models, fluency measures, and statistical properties. Second, we conducted a systematic architecture comparison evaluating multiple deep learning approaches on the same dataset and feature set. Third, we implemented efficient training procedures with GPU optimization including mixed precision training and tf.data pipelines. Fourth, we created a practical prediction system for quality estimation that can be used in real-world applications.</p>
</section>
<section id="main-findings" class="level2">
<h2 class="anchored" data-anchor-id="main-findings">Main Findings</h2>
<p>The analysis reveals that the Residual MLP achieved the best performance with the lowest MAE of 0.6430 and highest R² of 0.1475 among the tested architectures. The models successfully predict CCMatrix scores from sentence-level features, as evidenced by positive R² values and moderate Spearman correlations around 0.33-0.34. The feature engineering framework provides a solid foundation for quality estimation, and the results demonstrate that with adequate training data (100,000 samples), meaningful patterns can be learned. Further improvements may be achieved through additional features, more sophisticated architectures, or larger datasets.</p>
</section>
<section id="implications" class="level2">
<h2 class="anchored" data-anchor-id="implications">Implications</h2>
<p>The results suggest several important implications for translation quality estimation research. Sentence-level features demonstrate meaningful predictive power for CCMatrix alignment scores, with the Residual MLP achieving R² of 0.1475 and Spearman correlation of 0.3345, indicating that these features capture substantial information about alignment quality. However, there remains significant room for improvement, suggesting that additional information such as context, discourse structure, or pre-trained representations may further enhance performance. More sophisticated approaches including fine-tuned pre-trained models may be necessary to achieve performance comparable to state-of-the-art methods, though the current results establish a solid baseline. The task remains challenging but provides a strong foundation for future improvements, as the framework and methodology can be extended with additional features, architectures, and evaluation methods.</p>
</section>
<section id="final-remarks" class="level2">
<h2 class="anchored" data-anchor-id="final-remarks">Final Remarks</h2>
<p>While the current models show limited predictive power, this project establishes a framework for translation quality estimation that can be extended with larger datasets, advanced architectures such as fine-tuned transformers, additional features including linguistic and alignment-based properties, and human-annotated evaluation data. The codebase and methodology provide a solid foundation for future research in cross-lingual quality estimation, particularly for language pairs with substantial linguistic distance such as Hindi and Chinese.</p>
</section>
</section>
<section id="citations" class="level1">
<h1>Citations</h1>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p>The following references provide context and background for this work. Schwenk et al.&nbsp;(2021) introduced the CCMatrix dataset, which mines billions of high-quality parallel sentences from web-crawled data using cross-lingual embeddings. Reimers and Gurevych (2019) developed Sentence-BERT, which enables efficient computation of sentence embeddings using siamese BERT networks. Feng et al.&nbsp;(2022) created LaBSE, a language-agnostic BERT sentence embedding model that supports over 100 languages.</p>
<p>In the domain of translation quality estimation, Specia et al.&nbsp;(2013) provided foundational work on quality estimation for machine translation, while Fonseca et al.&nbsp;(2019) reported findings from the WMT 2019 Shared Task on Quality Estimation, establishing benchmarks for the field. Kim et al.&nbsp;(2017) developed a predictor-estimator approach using neural quality estimation based on target word prediction.</p>
<p>For technical implementation, Micikevicius et al.&nbsp;(2018) introduced mixed precision training, which enables faster computation while maintaining numerical stability. Abadi et al.&nbsp;(2016) developed TensorFlow, a system for large-scale machine learning that forms the foundation for our deep learning implementations.</p>
</section>
<section id="software-and-tools" class="level2">
<h2 class="anchored" data-anchor-id="software-and-tools">Software and Tools</h2>
<p>This project utilized Python 3.x as the primary programming language, TensorFlow and Keras for deep learning model development, scikit-learn for machine learning utilities and preprocessing, sentence-transformers for multilingual sentence embeddings, pandas and numpy for data manipulation and numerical computing, and matplotlib and seaborn for data visualization and analysis.</p>
<hr>
<p><em>This report was generated using Quarto. The code and data are available in the project repository.</em></p>
<!-- -->

</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb14" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Translation Quality Estimation for Hindi-Chinese Sentence Pairs"</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "A Deep Learning Approach to Predict Alignment Scores Without Reference Translations"</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "DSAN6600 Final Project"</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> today</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-depth: 3</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: show</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="co">    theme: cosmo</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-width: 10</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-height: 6</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-dpi: 300</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> references.bib</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rendering Instructions</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>To render this report, first generate training results by running <span class="in">`scripts/train_model.py`</span> to create <span class="in">`models/training_results.pkl`</span> and <span class="in">`models/training_summary.txt`</span>. Ensure that <span class="in">`pandas`</span>, <span class="in">`numpy`</span>, <span class="in">`matplotlib`</span>, and <span class="in">`seaborn`</span> are installed. Then render with Quarto by running <span class="in">`quarto render report.qmd`</span> in the project root directory. The report will automatically load and visualize training results if available.</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pickle</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">'ignore'</span>)</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Set style</span></span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>    plt.style.use(<span class="st">'seaborn-v0_8-darkgrid'</span>)</span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">OSError</span>:</span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>        plt.style.use(<span class="st">'seaborn-darkgrid'</span>)</span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">OSError</span>:</span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a>        plt.style.use(<span class="st">'default'</span>)</span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a>sns.set_palette(<span class="st">"husl"</span>)</span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a><span class="fu"># Introduction</span></span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a><span class="fu">## Problem Statement</span></span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a>Translation quality estimation (TQE) represents a critical task in natural language processing that aims to assess the quality of translated text without requiring reference translations. This capability is particularly valuable for machine translation systems that must automatically filter low-quality translations, for parallel corpus construction where high-quality sentence pairs must be identified for training data, for post-editing workflows that prioritize translations requiring human review, and for cross-lingual applications that evaluate alignment quality in multilingual datasets.</span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a>Traditional quality estimation methods often rely on reference translations, which are expensive and time-consuming to obtain. This project addresses the challenge of predicting translation quality scores for Hindi-Chinese sentence pairs using only source-target features, without access to reference translations.</span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a><span class="fu">## Motivation</span></span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a>Hindi and Chinese represent two of the world's most widely spoken languages, yet they belong to fundamentally different language families—Indo-European versus Sino-Tibetan—with distinct writing systems, grammatical structures, and cultural contexts. This substantial linguistic distance makes quality estimation particularly challenging but also scientifically interesting, as it tests the limits of cross-lingual feature extraction and neural network learning.</span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true" tabindex="-1"></a>The CCMatrix dataset provides alignment scores that reflect semantic similarity between sentence pairs, serving as a proxy for translation quality. By learning to predict these scores from sentence-level features, we can develop a system that automates quality assessment, reduces the need for manual evaluation, scales to large corpora processing millions of sentence pairs efficiently, provides interpretable features using linguistically motivated characteristics that can be analyzed, and generalizes across domains by learning patterns that may transfer to other language pairs.</span>
<span id="cb14-64"><a href="#cb14-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-65"><a href="#cb14-65" aria-hidden="true" tabindex="-1"></a><span class="fu">## Research Questions</span></span>
<span id="cb14-66"><a href="#cb14-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-67"><a href="#cb14-67" aria-hidden="true" tabindex="-1"></a>This project addresses four primary research questions. First, can we predict translation quality scores from source-target features alone? We investigate whether sentence-level features including length, semantic similarity, and fluency are sufficient to predict alignment scores. Second, which features are most predictive of translation quality? We analyze the contribution of different feature categories including length, semantic, and statistical properties to model performance. Third, how do different neural network architectures compare for this regression task? We systematically compare Simple MLP, Deep MLP, Residual MLP, and Transformer architectures. Fourth, what is the relationship between predicted scores and actual CCMatrix alignment scores? We evaluate correlation metrics including Pearson and Spearman correlations to assess prediction quality.</span>
<span id="cb14-68"><a href="#cb14-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-69"><a href="#cb14-69" aria-hidden="true" tabindex="-1"></a><span class="fu">## Objectives</span></span>
<span id="cb14-70"><a href="#cb14-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-71"><a href="#cb14-71" aria-hidden="true" tabindex="-1"></a>The primary objectives of this project are to develop a regression model that predicts continuous CCMatrix alignment scores in the 0.0-1.0 range, to engineer comprehensive features capturing length, semantic, statistical, and interaction properties, to compare multiple deep learning architectures to identify the best-performing model, to evaluate model performance using multiple metrics including MAE, RMSE, R², and correlation coefficients, and to provide a practical tool for quality estimation that can be used in real-world applications.</span>
<span id="cb14-72"><a href="#cb14-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-73"><a href="#cb14-73" aria-hidden="true" tabindex="-1"></a><span class="fu"># Methods</span></span>
<span id="cb14-74"><a href="#cb14-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-75"><a href="#cb14-75" aria-hidden="true" tabindex="-1"></a><span class="fu">## Dataset</span></span>
<span id="cb14-76"><a href="#cb14-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-77"><a href="#cb14-77" aria-hidden="true" tabindex="-1"></a><span class="fu">### Data Source</span></span>
<span id="cb14-78"><a href="#cb14-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-79"><a href="#cb14-79" aria-hidden="true" tabindex="-1"></a>The project utilizes the CCMatrix Hindi-Chinese parallel corpus, which contains sentence pairs extracted from web-crawled multilingual data. CCMatrix employs a cross-lingual embedding approach to compute alignment scores that reflect semantic similarity between sentence pairs. The dataset originates from the CC-Aligned project and consists of three files: <span class="in">`CCMatrix.hi-zh.hi`</span> containing Hindi sentences with one sentence per line, <span class="in">`CCMatrix.hi-zh.zh`</span> containing Chinese sentences with one sentence per line, and <span class="in">`CCMatrix.hi-zh.scores`</span> containing alignment scores as continuous values with one score per line.</span>
<span id="cb14-80"><a href="#cb14-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-81"><a href="#cb14-81" aria-hidden="true" tabindex="-1"></a><span class="fu">### Data Preprocessing</span></span>
<span id="cb14-82"><a href="#cb14-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-85"><a href="#cb14-85" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-86"><a href="#cb14-86" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb14-87"><a href="#cb14-87" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb14-88"><a href="#cb14-88" aria-hidden="true" tabindex="-1"></a><span class="co"># Load training summary to get dataset info</span></span>
<span id="cb14-89"><a href="#cb14-89" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb14-90"><a href="#cb14-90" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(<span class="st">'models/training_summary.txt'</span>, <span class="st">'r'</span>) <span class="im">as</span> f:</span>
<span id="cb14-91"><a href="#cb14-91" aria-hidden="true" tabindex="-1"></a>        summary_text <span class="op">=</span> f.read()</span>
<span id="cb14-92"><a href="#cb14-92" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Dataset Information from Training Summary:"</span>)</span>
<span id="cb14-93"><a href="#cb14-93" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb14-94"><a href="#cb14-94" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(summary_text)</span>
<span id="cb14-95"><a href="#cb14-95" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">FileNotFoundError</span>:</span>
<span id="cb14-96"><a href="#cb14-96" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Training summary not found. Please run train_model.py first."</span>)</span>
<span id="cb14-97"><a href="#cb14-97" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-98"><a href="#cb14-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-99"><a href="#cb14-99" aria-hidden="true" tabindex="-1"></a>The dataset preprocessing pipeline involves four main steps. First, sentence pairing matches Hindi and Chinese sentences by line number to ensure correspondence. Second, score extraction retrieves alignment scores from the scores file. Third, sampling applies a configurable sample size for training, with the current experiment utilizing 100,000 sentence pairs to provide sufficient data for robust model learning. Fourth, the data is split into training, validation, and test sets using a 70% / 15% / 15% partition, resulting in 70,000 training samples, 15,000 validation samples, and 15,000 test samples for model training and evaluation.</span>
<span id="cb14-100"><a href="#cb14-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-101"><a href="#cb14-101" aria-hidden="true" tabindex="-1"></a><span class="fu">### Target Variable</span></span>
<span id="cb14-102"><a href="#cb14-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-103"><a href="#cb14-103" aria-hidden="true" tabindex="-1"></a>The CCMatrix alignment score represents a continuous value typically ranging from 0.0 to 1.0, where higher scores closer to 1.0 indicate better alignment and semantic similarity, while lower scores closer to 0.0 indicate poor alignment or unrelated sentences. These scores are computed using cross-lingual embeddings and serve as a proxy for translation quality, though they measure semantic similarity rather than true translation accuracy.</span>
<span id="cb14-104"><a href="#cb14-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-105"><a href="#cb14-105" aria-hidden="true" tabindex="-1"></a><span class="fu">## Feature Engineering</span></span>
<span id="cb14-106"><a href="#cb14-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-107"><a href="#cb14-107" aria-hidden="true" tabindex="-1"></a>We engineer over 32 features across five distinct categories, each designed to capture different aspects of sentence pair relationships.</span>
<span id="cb14-108"><a href="#cb14-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-109"><a href="#cb14-109" aria-hidden="true" tabindex="-1"></a><span class="fu">### Length Features</span></span>
<span id="cb14-110"><a href="#cb14-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-111"><a href="#cb14-111" aria-hidden="true" tabindex="-1"></a>The first category encompasses nine length features that capture the relationship between sentence lengths in both languages. These include <span class="in">`hindi_length`</span> representing character count in Hindi sentences, <span class="in">`chinese_length`</span> representing character count in Chinese sentences, <span class="in">`hindi_word_count`</span> representing word count in Hindi sentences, <span class="in">`chinese_char_count`</span> representing Chinese character count specifically for Han characters, <span class="in">`length_ratio`</span> representing the ratio of Hindi to Chinese length, <span class="in">`length_diff`</span> representing absolute difference in lengths, <span class="in">`hindi_avg_word_length`</span> representing average word length in Hindi, <span class="in">`chinese_avg_char_length`</span> representing average character length in Chinese, and <span class="in">`length_ratio_normalized`</span> representing normalized length ratio. The rationale for these features stems from the observation that length mismatches can indicate translation quality issues, as well-aligned translations typically exhibit proportional lengths across languages.</span>
<span id="cb14-112"><a href="#cb14-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-113"><a href="#cb14-113" aria-hidden="true" tabindex="-1"></a><span class="fu">### Semantic Features</span></span>
<span id="cb14-114"><a href="#cb14-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-115"><a href="#cb14-115" aria-hidden="true" tabindex="-1"></a>The second category comprises eleven semantic features that capture semantic similarity and fluency. These include <span class="in">`embedding_similarity`</span> computed as cosine similarity using the <span class="in">`paraphrase-multilingual-MiniLM-L12-v2`</span> model, <span class="in">`labse_similarity`</span> computed as cosine similarity using the <span class="in">`sentence-transformers/LaBSE`</span> model, <span class="in">`chinese_perplexity`</span> representing language model perplexity as a fluency indicator, <span class="in">`chinese_fluency`</span> computed as the inverse of perplexity, <span class="in">`embedding_labse_interaction`</span> representing the product of embedding and LaBSE similarities, <span class="in">`similarity_avg`</span> representing the average of embedding and LaBSE similarities, <span class="in">`similarity_diff`</span> representing the absolute difference between similarities, <span class="in">`embedding_similarity_squared`</span> representing squared embedding similarity as a polynomial feature, <span class="in">`labse_similarity_squared`</span> representing squared LaBSE similarity as a polynomial feature, and <span class="in">`similarity_ratio`</span> representing the ratio of embedding to LaBSE similarity. The rationale for these features is that semantic similarity serves as a strong indicator of translation quality, and using multiple embedding models provides complementary signals that may capture different aspects of semantic equivalence.</span>
<span id="cb14-116"><a href="#cb14-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-117"><a href="#cb14-117" aria-hidden="true" tabindex="-1"></a><span class="fu">### Alignment Features</span></span>
<span id="cb14-118"><a href="#cb14-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-119"><a href="#cb14-119" aria-hidden="true" tabindex="-1"></a>The third category contains a single alignment feature: <span class="in">`char_alignment`</span> computed as a character-level alignment heuristic using Jaccard similarity of character sets. This feature captures surface-level similarity that may not be captured by semantic embeddings, particularly for languages with different writing systems where character overlap might indicate transliteration or proper noun correspondence.</span>
<span id="cb14-120"><a href="#cb14-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-121"><a href="#cb14-121" aria-hidden="true" tabindex="-1"></a><span class="fu">### Interaction Features</span></span>
<span id="cb14-122"><a href="#cb14-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-123"><a href="#cb14-123" aria-hidden="true" tabindex="-1"></a>The fourth category includes three interaction features that combine length and semantic information: <span class="in">`length_similarity_interaction`</span> representing the product of length ratio and embedding similarity, <span class="in">`length_perplexity_interaction`</span> representing the product of Chinese length and perplexity, and <span class="in">`length_perplexity_ratio`</span> representing the ratio of Chinese length to perplexity. These interaction features can capture non-linear relationships between features that may not be apparent in individual feature values.</span>
<span id="cb14-124"><a href="#cb14-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-125"><a href="#cb14-125" aria-hidden="true" tabindex="-1"></a><span class="fu">### Statistical Features</span></span>
<span id="cb14-126"><a href="#cb14-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-127"><a href="#cb14-127" aria-hidden="true" tabindex="-1"></a>The fifth category encompasses eight statistical features that capture text characteristics: <span class="in">`punctuation_count_hi`</span> and <span class="in">`punctuation_count_zh`</span> representing punctuation counts in Hindi and Chinese respectively, <span class="in">`digit_count_hi`</span> and <span class="in">`digit_count_zh`</span> representing digit counts in both languages, <span class="in">`punctuation_ratio_hi`</span> and <span class="in">`punctuation_ratio_zh`</span> representing punctuation ratios normalized by sentence length, and <span class="in">`vocab_diversity_hi`</span> and <span class="in">`vocab_diversity_zh`</span> representing vocabulary diversity computed as the ratio of unique tokens to total tokens. These statistical properties can indicate translation quality and text characteristics that may correlate with alignment scores.</span>
<span id="cb14-128"><a href="#cb14-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-129"><a href="#cb14-129" aria-hidden="true" tabindex="-1"></a><span class="fu">### Feature Selection</span></span>
<span id="cb14-130"><a href="#cb14-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-131"><a href="#cb14-131" aria-hidden="true" tabindex="-1"></a>After feature engineering, we apply colinearity removal to eliminate highly correlated features with correlation coefficients exceeding 0.95. This process reduces redundancy and improves model generalization by preventing multicollinearity issues. The feature selection step reduced the feature set from 31 to 25 features in the current experiment, removing 6 highly correlated features including embedding similarity squared, Hindi word count, LaBSE similarity squared, length difference, normalized length ratio, and similarity average.</span>
<span id="cb14-132"><a href="#cb14-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-133"><a href="#cb14-133" aria-hidden="true" tabindex="-1"></a><span class="fu">## Model Architectures</span></span>
<span id="cb14-134"><a href="#cb14-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-135"><a href="#cb14-135" aria-hidden="true" tabindex="-1"></a>We compare four deep learning architectures, each representing different approaches to learning from the engineered features.</span>
<span id="cb14-136"><a href="#cb14-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-137"><a href="#cb14-137" aria-hidden="true" tabindex="-1"></a><span class="fu">### Simple MLP</span></span>
<span id="cb14-138"><a href="#cb14-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-139"><a href="#cb14-139" aria-hidden="true" tabindex="-1"></a>The first architecture is a baseline multi-layer perceptron with regularization. The architecture consists of an input layer followed by a dense layer with 128 units and ReLU activation, dropout with rate 0.3, a second dense layer with 64 units and ReLU activation, dropout with rate 0.2, and a final dense output layer with a single linear unit. Regularization includes L2 weight decay with coefficient 0.01 and dropout layers. This model contains approximately 20,000 parameters and serves as a baseline for comparison.</span>
<span id="cb14-140"><a href="#cb14-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-141"><a href="#cb14-141" aria-hidden="true" tabindex="-1"></a><span class="fu">### Deep MLP</span></span>
<span id="cb14-142"><a href="#cb14-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-143"><a href="#cb14-143" aria-hidden="true" tabindex="-1"></a>The second architecture is a deeper network with batch normalization. The architecture consists of an input layer followed by a dense layer with 256 units and ReLU activation, batch normalization, dropout with rate 0.4, a second dense layer with 128 units and ReLU activation, batch normalization, dropout with rate 0.3, a third dense layer with 64 units and ReLU activation, dropout with rate 0.2, and a final dense output layer with a single linear unit. Regularization includes L2 weight decay with coefficient 0.01, batch normalization layers, and dropout with rates ranging from 0.2 to 0.4. This model contains approximately 100,000 parameters.</span>
<span id="cb14-144"><a href="#cb14-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-145"><a href="#cb14-145" aria-hidden="true" tabindex="-1"></a><span class="fu">### Residual MLP</span></span>
<span id="cb14-146"><a href="#cb14-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-147"><a href="#cb14-147" aria-hidden="true" tabindex="-1"></a>The third architecture is a residual network with skip connections. The architecture is similar to the Deep MLP but incorporates residual connections between layers, allowing gradients to flow more easily through the network and potentially enabling training of deeper architectures. Regularization includes L2 weight decay with coefficient 0.01, batch normalization layers, and dropout with rates ranging from 0.2 to 0.4. This model contains approximately 120,000 parameters.</span>
<span id="cb14-148"><a href="#cb14-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-149"><a href="#cb14-149" aria-hidden="true" tabindex="-1"></a><span class="fu">### Cross-Attention Transformer</span></span>
<span id="cb14-150"><a href="#cb14-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-151"><a href="#cb14-151" aria-hidden="true" tabindex="-1"></a>The fourth architecture is a transformer model with cross-attention between Hindi and Chinese embeddings. The architecture includes separate input projections for Hindi and Chinese embeddings, multi-head cross-attention layers enabling bidirectional attention between Hindi and Chinese representations, feed-forward networks with layer normalization, and concatenation followed by final dense layers. Hyperparameters include 8 attention heads, 2 transformer layers, and a model dimension of 256. This model contains approximately 200,000 parameters and represents the most sophisticated architecture tested.</span>
<span id="cb14-152"><a href="#cb14-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-153"><a href="#cb14-153" aria-hidden="true" tabindex="-1"></a><span class="fu">## Training Procedure</span></span>
<span id="cb14-154"><a href="#cb14-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-155"><a href="#cb14-155" aria-hidden="true" tabindex="-1"></a><span class="fu">### Data Preprocessing</span></span>
<span id="cb14-156"><a href="#cb14-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-157"><a href="#cb14-157" aria-hidden="true" tabindex="-1"></a>The training procedure begins with comprehensive data preprocessing. Feature scaling applies StandardScaler to all features, transforming them to have zero mean and unit variance. Target scaling applies StandardScaler to CCMatrix scores to improve neural network training stability. During evaluation, predictions are inverse-transformed back to the original scale to compute metrics on the natural score range.</span>
<span id="cb14-158"><a href="#cb14-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-159"><a href="#cb14-159" aria-hidden="true" tabindex="-1"></a><span class="fu">### Training Configuration</span></span>
<span id="cb14-160"><a href="#cb14-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-161"><a href="#cb14-161" aria-hidden="true" tabindex="-1"></a>Training employs the Adam optimizer with default learning rate settings. The loss function is Mean Squared Error (MSE), with Mean Absolute Error (MAE) tracked as an additional metric. Batch size is set to 256 for GPU training or 64 for CPU training, depending on hardware availability. Training proceeds for up to 100 epochs with early stopping based on validation loss. Early stopping patience is set to 10 epochs for MLP architectures and 5 epochs for the transformer architecture. Mixed precision training with FP16 is enabled on GPU for faster computation while maintaining numerical stability in the output layer through FP32 precision.</span>
<span id="cb14-162"><a href="#cb14-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-163"><a href="#cb14-163" aria-hidden="true" tabindex="-1"></a><span class="fu">### GPU Optimization</span></span>
<span id="cb14-164"><a href="#cb14-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-165"><a href="#cb14-165" aria-hidden="true" tabindex="-1"></a>The training script incorporates several GPU optimizations to maximize computational efficiency. Memory growth enables dynamic GPU memory allocation to avoid allocating all GPU memory at once. Graph mode utilizes TensorFlow graph execution for faster computation compared to eager execution. Mixed precision training employs FP16 for intermediate computations while maintaining FP32 for the output layer. The tf.data pipeline provides efficient data loading with prefetching and batching to minimize data loading bottlenecks during training.</span>
<span id="cb14-166"><a href="#cb14-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-167"><a href="#cb14-167" aria-hidden="true" tabindex="-1"></a><span class="fu">### Evaluation Metrics</span></span>
<span id="cb14-168"><a href="#cb14-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-169"><a href="#cb14-169" aria-hidden="true" tabindex="-1"></a>Models are evaluated using multiple metrics to provide comprehensive performance assessment. Mean Absolute Error (MAE) measures average prediction error and is the primary metric for model selection. Root Mean Squared Error (RMSE) penalizes larger errors more heavily than smaller errors. The coefficient of determination (R²) measures the proportion of variance in the target variable explained by the model. Pearson correlation measures linear relationship strength between predictions and actual scores. Spearman correlation measures monotonic relationship strength using rank-based correlation, which is less sensitive to outliers than Pearson correlation.</span>
<span id="cb14-170"><a href="#cb14-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-171"><a href="#cb14-171" aria-hidden="true" tabindex="-1"></a><span class="fu"># Analyses</span></span>
<span id="cb14-172"><a href="#cb14-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-173"><a href="#cb14-173" aria-hidden="true" tabindex="-1"></a><span class="fu">## Data Exploration</span></span>
<span id="cb14-174"><a href="#cb14-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-177"><a href="#cb14-177" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-178"><a href="#cb14-178" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-179"><a href="#cb14-179" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb14-180"><a href="#cb14-180" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Distribution of CCMatrix alignment scores in the dataset"</span></span>
<span id="cb14-181"><a href="#cb14-181" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 10</span></span>
<span id="cb14-182"><a href="#cb14-182" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb14-183"><a href="#cb14-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-184"><a href="#cb14-184" aria-hidden="true" tabindex="-1"></a><span class="co"># Load training results</span></span>
<span id="cb14-185"><a href="#cb14-185" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb14-186"><a href="#cb14-186" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(<span class="st">'models/training_results.pkl'</span>, <span class="st">'rb'</span>) <span class="im">as</span> f:</span>
<span id="cb14-187"><a href="#cb14-187" aria-hidden="true" tabindex="-1"></a>        results <span class="op">=</span> pickle.load(f)</span>
<span id="cb14-188"><a href="#cb14-188" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-189"><a href="#cb14-189" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load training summary</span></span>
<span id="cb14-190"><a href="#cb14-190" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(<span class="st">'models/training_summary.txt'</span>, <span class="st">'r'</span>) <span class="im">as</span> f:</span>
<span id="cb14-191"><a href="#cb14-191" aria-hidden="true" tabindex="-1"></a>        summary_lines <span class="op">=</span> f.readlines()</span>
<span id="cb14-192"><a href="#cb14-192" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-193"><a href="#cb14-193" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Training Results Summary:"</span>)</span>
<span id="cb14-194"><a href="#cb14-194" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb14-195"><a href="#cb14-195" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> model_name, metrics <span class="kw">in</span> results.items():</span>
<span id="cb14-196"><a href="#cb14-196" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>model_name<span class="sc">.</span>upper()<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb14-197"><a href="#cb14-197" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Test MAE: </span><span class="sc">{</span>metrics[<span class="st">'test_mae'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb14-198"><a href="#cb14-198" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Test RMSE: </span><span class="sc">{</span>metrics[<span class="st">'test_rmse'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb14-199"><a href="#cb14-199" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Test R²: </span><span class="sc">{</span>metrics[<span class="st">'test_r2'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb14-200"><a href="#cb14-200" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> metrics.get(<span class="st">'test_spearman'</span>) <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb14-201"><a href="#cb14-201" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"  Test Spearman: </span><span class="sc">{</span>metrics[<span class="st">'test_spearman'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb14-202"><a href="#cb14-202" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> metrics.get(<span class="st">'test_pearson'</span>) <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb14-203"><a href="#cb14-203" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"  Test Pearson: </span><span class="sc">{</span>metrics[<span class="st">'test_pearson'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb14-204"><a href="#cb14-204" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-205"><a href="#cb14-205" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create comparison plot</span></span>
<span id="cb14-206"><a href="#cb14-206" aria-hidden="true" tabindex="-1"></a>    models <span class="op">=</span> <span class="bu">list</span>(results.keys())</span>
<span id="cb14-207"><a href="#cb14-207" aria-hidden="true" tabindex="-1"></a>    mae_values <span class="op">=</span> [results[m][<span class="st">'test_mae'</span>] <span class="cf">for</span> m <span class="kw">in</span> models]</span>
<span id="cb14-208"><a href="#cb14-208" aria-hidden="true" tabindex="-1"></a>    rmse_values <span class="op">=</span> [results[m][<span class="st">'test_rmse'</span>] <span class="cf">for</span> m <span class="kw">in</span> models]</span>
<span id="cb14-209"><a href="#cb14-209" aria-hidden="true" tabindex="-1"></a>    r2_values <span class="op">=</span> [results[m][<span class="st">'test_r2'</span>] <span class="cf">for</span> m <span class="kw">in</span> models]</span>
<span id="cb14-210"><a href="#cb14-210" aria-hidden="true" tabindex="-1"></a>    spearman_values <span class="op">=</span> [results[m].get(<span class="st">'test_spearman'</span>, <span class="dv">0</span>) <span class="cf">if</span> results[m].get(<span class="st">'test_spearman'</span>) <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> <span class="dv">0</span> <span class="cf">for</span> m <span class="kw">in</span> models]</span>
<span id="cb14-211"><a href="#cb14-211" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-212"><a href="#cb14-212" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">10</span>))</span>
<span id="cb14-213"><a href="#cb14-213" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-214"><a href="#cb14-214" aria-hidden="true" tabindex="-1"></a>    <span class="co"># MAE comparison</span></span>
<span id="cb14-215"><a href="#cb14-215" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>, <span class="dv">0</span>].bar(models, mae_values, color<span class="op">=</span>[<span class="st">'#3498db'</span>, <span class="st">'#e74c3c'</span>, <span class="st">'#2ecc71'</span>, <span class="st">'#f39c12'</span>])</span>
<span id="cb14-216"><a href="#cb14-216" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>, <span class="dv">0</span>].set_title(<span class="st">'Test MAE by Model'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb14-217"><a href="#cb14-217" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>, <span class="dv">0</span>].set_ylabel(<span class="st">'MAE'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb14-218"><a href="#cb14-218" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>, <span class="dv">0</span>].tick_params(axis<span class="op">=</span><span class="st">'x'</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb14-219"><a href="#cb14-219" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>, <span class="dv">0</span>].grid(axis<span class="op">=</span><span class="st">'y'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb14-220"><a href="#cb14-220" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-221"><a href="#cb14-221" aria-hidden="true" tabindex="-1"></a>    <span class="co"># RMSE comparison</span></span>
<span id="cb14-222"><a href="#cb14-222" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>, <span class="dv">1</span>].bar(models, rmse_values, color<span class="op">=</span>[<span class="st">'#3498db'</span>, <span class="st">'#e74c3c'</span>, <span class="st">'#2ecc71'</span>, <span class="st">'#f39c12'</span>])</span>
<span id="cb14-223"><a href="#cb14-223" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>, <span class="dv">1</span>].set_title(<span class="st">'Test RMSE by Model'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb14-224"><a href="#cb14-224" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>, <span class="dv">1</span>].set_ylabel(<span class="st">'RMSE'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb14-225"><a href="#cb14-225" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>, <span class="dv">1</span>].tick_params(axis<span class="op">=</span><span class="st">'x'</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb14-226"><a href="#cb14-226" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>, <span class="dv">1</span>].grid(axis<span class="op">=</span><span class="st">'y'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb14-227"><a href="#cb14-227" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-228"><a href="#cb14-228" aria-hidden="true" tabindex="-1"></a>    <span class="co"># R² comparison</span></span>
<span id="cb14-229"><a href="#cb14-229" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>, <span class="dv">0</span>].bar(models, r2_values, color<span class="op">=</span>[<span class="st">'#3498db'</span>, <span class="st">'#e74c3c'</span>, <span class="st">'#2ecc71'</span>, <span class="st">'#f39c12'</span>])</span>
<span id="cb14-230"><a href="#cb14-230" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>, <span class="dv">0</span>].set_title(<span class="st">'Test R² by Model'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb14-231"><a href="#cb14-231" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>, <span class="dv">0</span>].set_ylabel(<span class="st">'R²'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb14-232"><a href="#cb14-232" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>, <span class="dv">0</span>].tick_params(axis<span class="op">=</span><span class="st">'x'</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb14-233"><a href="#cb14-233" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>, <span class="dv">0</span>].grid(axis<span class="op">=</span><span class="st">'y'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb14-234"><a href="#cb14-234" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>, <span class="dv">0</span>].axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb14-235"><a href="#cb14-235" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-236"><a href="#cb14-236" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Spearman correlation comparison</span></span>
<span id="cb14-237"><a href="#cb14-237" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>, <span class="dv">1</span>].bar(models, spearman_values, color<span class="op">=</span>[<span class="st">'#3498db'</span>, <span class="st">'#e74c3c'</span>, <span class="st">'#2ecc71'</span>, <span class="st">'#f39c12'</span>])</span>
<span id="cb14-238"><a href="#cb14-238" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>, <span class="dv">1</span>].set_title(<span class="st">'Test Spearman Correlation by Model'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb14-239"><a href="#cb14-239" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>, <span class="dv">1</span>].set_ylabel(<span class="st">'Spearman Correlation'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb14-240"><a href="#cb14-240" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>, <span class="dv">1</span>].tick_params(axis<span class="op">=</span><span class="st">'x'</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb14-241"><a href="#cb14-241" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>, <span class="dv">1</span>].grid(axis<span class="op">=</span><span class="st">'y'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb14-242"><a href="#cb14-242" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>, <span class="dv">1</span>].axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb14-243"><a href="#cb14-243" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-244"><a href="#cb14-244" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb14-245"><a href="#cb14-245" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb14-246"><a href="#cb14-246" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-247"><a href="#cb14-247" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">FileNotFoundError</span>:</span>
<span id="cb14-248"><a href="#cb14-248" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Training results not found. Please run train_model.py first to generate results."</span>)</span>
<span id="cb14-249"><a href="#cb14-249" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-250"><a href="#cb14-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-251"><a href="#cb14-251" aria-hidden="true" tabindex="-1"></a><span class="fu">## Model Performance Comparison</span></span>
<span id="cb14-252"><a href="#cb14-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-255"><a href="#cb14-255" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-256"><a href="#cb14-256" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-257"><a href="#cb14-257" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb14-258"><a href="#cb14-258" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Comprehensive model performance comparison across all metrics"</span></span>
<span id="cb14-259"><a href="#cb14-259" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 12</span></span>
<span id="cb14-260"><a href="#cb14-260" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 8</span></span>
<span id="cb14-261"><a href="#cb14-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-262"><a href="#cb14-262" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb14-263"><a href="#cb14-263" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a comprehensive comparison table</span></span>
<span id="cb14-264"><a href="#cb14-264" aria-hidden="true" tabindex="-1"></a>    comparison_data <span class="op">=</span> []</span>
<span id="cb14-265"><a href="#cb14-265" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> model_name, metrics <span class="kw">in</span> results.items():</span>
<span id="cb14-266"><a href="#cb14-266" aria-hidden="true" tabindex="-1"></a>        comparison_data.append({</span>
<span id="cb14-267"><a href="#cb14-267" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Model'</span>: model_name.replace(<span class="st">'_'</span>, <span class="st">' '</span>).title(),</span>
<span id="cb14-268"><a href="#cb14-268" aria-hidden="true" tabindex="-1"></a>            <span class="st">'MAE'</span>: metrics[<span class="st">'test_mae'</span>],</span>
<span id="cb14-269"><a href="#cb14-269" aria-hidden="true" tabindex="-1"></a>            <span class="st">'RMSE'</span>: metrics[<span class="st">'test_rmse'</span>],</span>
<span id="cb14-270"><a href="#cb14-270" aria-hidden="true" tabindex="-1"></a>            <span class="st">'R²'</span>: metrics[<span class="st">'test_r2'</span>],</span>
<span id="cb14-271"><a href="#cb14-271" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Spearman'</span>: metrics.get(<span class="st">'test_spearman'</span>, np.nan),</span>
<span id="cb14-272"><a href="#cb14-272" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Pearson'</span>: metrics.get(<span class="st">'test_pearson'</span>, np.nan),</span>
<span id="cb14-273"><a href="#cb14-273" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Train MAE'</span>: metrics.get(<span class="st">'train_mae'</span>, np.nan),</span>
<span id="cb14-274"><a href="#cb14-274" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Val MAE'</span>: metrics.get(<span class="st">'val_mae'</span>, np.nan)</span>
<span id="cb14-275"><a href="#cb14-275" aria-hidden="true" tabindex="-1"></a>        })</span>
<span id="cb14-276"><a href="#cb14-276" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-277"><a href="#cb14-277" aria-hidden="true" tabindex="-1"></a>    df_comparison <span class="op">=</span> pd.DataFrame(comparison_data)</span>
<span id="cb14-278"><a href="#cb14-278" aria-hidden="true" tabindex="-1"></a>    df_comparison <span class="op">=</span> df_comparison.sort_values(<span class="st">'MAE'</span>)</span>
<span id="cb14-279"><a href="#cb14-279" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-280"><a href="#cb14-280" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Model Performance Comparison Table:"</span>)</span>
<span id="cb14-281"><a href="#cb14-281" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb14-282"><a href="#cb14-282" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(df_comparison.to_string(index<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb14-283"><a href="#cb14-283" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-284"><a href="#cb14-284" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create heatmap of metrics</span></span>
<span id="cb14-285"><a href="#cb14-285" aria-hidden="true" tabindex="-1"></a>    metrics_for_heatmap <span class="op">=</span> df_comparison.set_index(<span class="st">'Model'</span>)[[<span class="st">'MAE'</span>, <span class="st">'RMSE'</span>, <span class="st">'R²'</span>, <span class="st">'Spearman'</span>, <span class="st">'Pearson'</span>]]</span>
<span id="cb14-286"><a href="#cb14-286" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-287"><a href="#cb14-287" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb14-288"><a href="#cb14-288" aria-hidden="true" tabindex="-1"></a>    sns.heatmap(metrics_for_heatmap.T, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">'.4f'</span>, cmap<span class="op">=</span><span class="st">'RdYlGn_r'</span>, </span>
<span id="cb14-289"><a href="#cb14-289" aria-hidden="true" tabindex="-1"></a>                center<span class="op">=</span><span class="dv">0</span>, vmin<span class="op">=-</span><span class="fl">0.2</span>, vmax<span class="op">=</span><span class="fl">1.2</span>, cbar_kws<span class="op">=</span>{<span class="st">'label'</span>: <span class="st">'Metric Value'</span>}, ax<span class="op">=</span>ax)</span>
<span id="cb14-290"><a href="#cb14-290" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="st">'Model Performance Heatmap'</span>, fontsize<span class="op">=</span><span class="dv">16</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>, pad<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb14-291"><a href="#cb14-291" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">'Model'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb14-292"><a href="#cb14-292" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">'Metric'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb14-293"><a href="#cb14-293" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb14-294"><a href="#cb14-294" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb14-295"><a href="#cb14-295" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-296"><a href="#cb14-296" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">NameError</span>:</span>
<span id="cb14-297"><a href="#cb14-297" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Results not loaded. Please run the previous code chunk first."</span>)</span>
<span id="cb14-298"><a href="#cb14-298" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-299"><a href="#cb14-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-300"><a href="#cb14-300" aria-hidden="true" tabindex="-1"></a><span class="fu">## Training Curves</span></span>
<span id="cb14-301"><a href="#cb14-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-304"><a href="#cb14-304" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-305"><a href="#cb14-305" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-306"><a href="#cb14-306" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb14-307"><a href="#cb14-307" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Training and validation loss curves for all models (if available)"</span></span>
<span id="cb14-308"><a href="#cb14-308" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 12</span></span>
<span id="cb14-309"><a href="#cb14-309" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 8</span></span>
<span id="cb14-310"><a href="#cb14-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-311"><a href="#cb14-311" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: Training history is not saved by default in the current implementation</span></span>
<span id="cb14-312"><a href="#cb14-312" aria-hidden="true" tabindex="-1"></a><span class="co"># This is a placeholder for when training history is available</span></span>
<span id="cb14-313"><a href="#cb14-313" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Note: Training history (loss curves) would be displayed here if saved during training."</span>)</span>
<span id="cb14-314"><a href="#cb14-314" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"To enable this, modify train_model.py to save training history dictionaries."</span>)</span>
<span id="cb14-315"><a href="#cb14-315" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Current implementation saves final metrics but not epoch-by-epoch training curves."</span>)</span>
<span id="cb14-316"><a href="#cb14-316" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-317"><a href="#cb14-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-318"><a href="#cb14-318" aria-hidden="true" tabindex="-1"></a><span class="fu">## Best Model Analysis</span></span>
<span id="cb14-319"><a href="#cb14-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-322"><a href="#cb14-322" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-323"><a href="#cb14-323" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-324"><a href="#cb14-324" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb14-325"><a href="#cb14-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-326"><a href="#cb14-326" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb14-327"><a href="#cb14-327" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Find best model (lowest MAE)</span></span>
<span id="cb14-328"><a href="#cb14-328" aria-hidden="true" tabindex="-1"></a>    best_model_name <span class="op">=</span> <span class="bu">min</span>(results.keys(), key<span class="op">=</span><span class="kw">lambda</span> k: results[k][<span class="st">'test_mae'</span>])</span>
<span id="cb14-329"><a href="#cb14-329" aria-hidden="true" tabindex="-1"></a>    best_metrics <span class="op">=</span> results[best_model_name]</span>
<span id="cb14-330"><a href="#cb14-330" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-331"><a href="#cb14-331" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb14-332"><a href="#cb14-332" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"BEST MODEL ANALYSIS"</span>)</span>
<span id="cb14-333"><a href="#cb14-333" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb14-334"><a href="#cb14-334" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Best Model: </span><span class="sc">{</span>best_model_name<span class="sc">.</span>upper()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-335"><a href="#cb14-335" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Performance Metrics:"</span>)</span>
<span id="cb14-336"><a href="#cb14-336" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Test MAE:  </span><span class="sc">{</span>best_metrics[<span class="st">'test_mae'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb14-337"><a href="#cb14-337" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Test RMSE: </span><span class="sc">{</span>best_metrics[<span class="st">'test_rmse'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb14-338"><a href="#cb14-338" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Test R²:   </span><span class="sc">{</span>best_metrics[<span class="st">'test_r2'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb14-339"><a href="#cb14-339" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> best_metrics.get(<span class="st">'test_spearman'</span>) <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb14-340"><a href="#cb14-340" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Test Spearman: </span><span class="sc">{</span>best_metrics[<span class="st">'test_spearman'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb14-341"><a href="#cb14-341" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> best_metrics.get(<span class="st">'test_pearson'</span>) <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb14-342"><a href="#cb14-342" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Test Pearson:  </span><span class="sc">{</span>best_metrics[<span class="st">'test_pearson'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb14-343"><a href="#cb14-343" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-344"><a href="#cb14-344" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Training Metrics:"</span>)</span>
<span id="cb14-345"><a href="#cb14-345" aria-hidden="true" tabindex="-1"></a>    train_mae <span class="op">=</span> best_metrics.get(<span class="st">'train_mae'</span>)</span>
<span id="cb14-346"><a href="#cb14-346" aria-hidden="true" tabindex="-1"></a>    val_mae <span class="op">=</span> best_metrics.get(<span class="st">'val_mae'</span>)</span>
<span id="cb14-347"><a href="#cb14-347" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Train MAE: </span><span class="sc">{</span>train_mae<span class="sc">:.4f}</span><span class="ss">"</span> <span class="cf">if</span> train_mae <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> <span class="st">"  Train MAE: N/A"</span>)</span>
<span id="cb14-348"><a href="#cb14-348" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Val MAE:   </span><span class="sc">{</span>val_mae<span class="sc">:.4f}</span><span class="ss">"</span> <span class="cf">if</span> val_mae <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> <span class="st">"  Val MAE:   N/A"</span>)</span>
<span id="cb14-349"><a href="#cb14-349" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-350"><a href="#cb14-350" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Interpretation</span></span>
<span id="cb14-351"><a href="#cb14-351" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Interpretation:"</span>)</span>
<span id="cb14-352"><a href="#cb14-352" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> best_metrics[<span class="st">'test_r2'</span>] <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb14-353"><a href="#cb14-353" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  ✓ The model explains </span><span class="sc">{</span>best_metrics[<span class="st">'test_r2'</span>]<span class="op">*</span><span class="dv">100</span><span class="sc">:.1f}</span><span class="ss">% of variance in CCMatrix scores"</span>)</span>
<span id="cb14-354"><a href="#cb14-354" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb14-355"><a href="#cb14-355" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  ⚠ The model performs worse than a simple mean baseline (R² &lt; 0)"</span>)</span>
<span id="cb14-356"><a href="#cb14-356" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-357"><a href="#cb14-357" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> best_metrics.get(<span class="st">'test_spearman'</span>) <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb14-358"><a href="#cb14-358" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">abs</span>(best_metrics[<span class="st">'test_spearman'</span>]) <span class="op">&gt;</span> <span class="fl">0.5</span>:</span>
<span id="cb14-359"><a href="#cb14-359" aria-hidden="true" tabindex="-1"></a>            strength <span class="op">=</span> <span class="st">"strong"</span> <span class="cf">if</span> <span class="bu">abs</span>(best_metrics[<span class="st">'test_spearman'</span>]) <span class="op">&gt;</span> <span class="fl">0.7</span> <span class="cf">else</span> <span class="st">"moderate"</span></span>
<span id="cb14-360"><a href="#cb14-360" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"  ✓ </span><span class="sc">{</span>strength<span class="sc">.</span>capitalize()<span class="sc">}</span><span class="ss"> monotonic relationship (Spearman = </span><span class="sc">{</span>best_metrics[<span class="st">'test_spearman'</span>]<span class="sc">:.3f}</span><span class="ss">)"</span>)</span>
<span id="cb14-361"><a href="#cb14-361" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb14-362"><a href="#cb14-362" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"  ⚠ Weak monotonic relationship (Spearman = </span><span class="sc">{</span>best_metrics[<span class="st">'test_spearman'</span>]<span class="sc">:.3f}</span><span class="ss">)"</span>)</span>
<span id="cb14-363"><a href="#cb14-363" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-364"><a href="#cb14-364" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">NameError</span>:</span>
<span id="cb14-365"><a href="#cb14-365" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Results not loaded. Please run the previous code chunks first."</span>)</span>
<span id="cb14-366"><a href="#cb14-366" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-367"><a href="#cb14-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-368"><a href="#cb14-368" aria-hidden="true" tabindex="-1"></a><span class="fu">## Feature Importance Analysis</span></span>
<span id="cb14-369"><a href="#cb14-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-372"><a href="#cb14-372" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-373"><a href="#cb14-373" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-374"><a href="#cb14-374" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb14-375"><a href="#cb14-375" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Note: Feature importance analysis would require additional computation"</span></span>
<span id="cb14-376"><a href="#cb14-376" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 10</span></span>
<span id="cb14-377"><a href="#cb14-377" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb14-378"><a href="#cb14-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-379"><a href="#cb14-379" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Feature Importance Analysis:"</span>)</span>
<span id="cb14-380"><a href="#cb14-380" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb14-381"><a href="#cb14-381" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Neural network models (MLP, Deep MLP, Residual MLP, Transformer) learn"</span>)</span>
<span id="cb14-382"><a href="#cb14-382" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"distributed representations across all features, making direct feature"</span>)</span>
<span id="cb14-383"><a href="#cb14-383" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"importance extraction challenging."</span>)</span>
<span id="cb14-384"><a href="#cb14-384" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">To analyze feature importance, consider:"</span>)</span>
<span id="cb14-385"><a href="#cb14-385" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  1. Permutation importance: Shuffle each feature and measure performance drop"</span>)</span>
<span id="cb14-386"><a href="#cb14-386" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  2. SHAP values: Compute Shapley Additive Explanations"</span>)</span>
<span id="cb14-387"><a href="#cb14-387" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  3. Gradient-based methods: Analyze gradients w.r.t. input features"</span>)</span>
<span id="cb14-388"><a href="#cb14-388" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  4. Ablation studies: Remove feature categories and measure impact"</span>)</span>
<span id="cb14-389"><a href="#cb14-389" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-390"><a href="#cb14-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-391"><a href="#cb14-391" aria-hidden="true" tabindex="-1"></a><span class="fu">## Error Analysis</span></span>
<span id="cb14-392"><a href="#cb14-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-395"><a href="#cb14-395" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-396"><a href="#cb14-396" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-397"><a href="#cb14-397" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb14-398"><a href="#cb14-398" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Error distribution analysis (placeholder - requires prediction data)"</span></span>
<span id="cb14-399"><a href="#cb14-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-400"><a href="#cb14-400" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Error Analysis:"</span>)</span>
<span id="cb14-401"><a href="#cb14-401" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb14-402"><a href="#cb14-402" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">To perform detailed error analysis, we would need:"</span>)</span>
<span id="cb14-403"><a href="#cb14-403" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  1. Test set predictions from the best model"</span>)</span>
<span id="cb14-404"><a href="#cb14-404" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  2. Actual CCMatrix scores"</span>)</span>
<span id="cb14-405"><a href="#cb14-405" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  3. Sentence pairs for qualitative analysis"</span>)</span>
<span id="cb14-406"><a href="#cb14-406" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">This would allow us to:"</span>)</span>
<span id="cb14-407"><a href="#cb14-407" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  - Identify high-error cases"</span>)</span>
<span id="cb14-408"><a href="#cb14-408" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  - Analyze error patterns (over/under-prediction)"</span>)</span>
<span id="cb14-409"><a href="#cb14-409" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  - Examine sentence characteristics of misclassified pairs"</span>)</span>
<span id="cb14-410"><a href="#cb14-410" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  - Generate scatter plots: predicted vs. actual scores"</span>)</span>
<span id="cb14-411"><a href="#cb14-411" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-412"><a href="#cb14-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-413"><a href="#cb14-413" aria-hidden="true" tabindex="-1"></a><span class="fu"># Discussion</span></span>
<span id="cb14-414"><a href="#cb14-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-415"><a href="#cb14-415" aria-hidden="true" tabindex="-1"></a><span class="fu">## Model Performance Interpretation</span></span>
<span id="cb14-416"><a href="#cb14-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-417"><a href="#cb14-417" aria-hidden="true" tabindex="-1"></a><span class="fu">### Overall Performance</span></span>
<span id="cb14-418"><a href="#cb14-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-419"><a href="#cb14-419" aria-hidden="true" tabindex="-1"></a>The models achieve improved performance on the translation quality estimation task when trained on the larger dataset. The best model, identified as Residual MLP, achieves a mean absolute error of 0.6430 on the CCMatrix score scale, a positive R² value of 0.1475 indicating that the model explains approximately 14.75% of variance in alignment scores, and a Spearman correlation of 0.3345 indicating a moderate monotonic relationship between predictions and actual scores. These results represent substantial improvement over baseline performance and demonstrate that the models successfully learn meaningful patterns from the engineered features.</span>
<span id="cb14-420"><a href="#cb14-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-421"><a href="#cb14-421" aria-hidden="true" tabindex="-1"></a><span class="fu">### Key Findings</span></span>
<span id="cb14-422"><a href="#cb14-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-423"><a href="#cb14-423" aria-hidden="true" tabindex="-1"></a>The analysis reveals several important findings. First, the models demonstrate meaningful predictive power when trained on the larger dataset of 100,000 samples, as evidenced by positive R² values of approximately 0.14-0.15, indicating that the models explain a significant portion of variance in CCMatrix alignment scores. This represents a substantial improvement over baseline performance and demonstrates that sentence-level features can successfully predict alignment scores, though the relationship remains challenging with room for further improvement.</span>
<span id="cb14-424"><a href="#cb14-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-425"><a href="#cb14-425" aria-hidden="true" tabindex="-1"></a>Second, the model architecture comparison reveals that the Residual MLP performs best with the lowest MAE of 0.6430 and highest R² of 0.1475, suggesting that residual connections enable more effective learning of complex patterns in the larger dataset. The Deep MLP achieves similar performance with MAE of 0.6475 and R² of 0.1442, while the Simple MLP provides a reasonable baseline with MAE of 0.6488 and R² of 0.1446. The Transformer architecture achieves lower performance with MAE of 0.6846 and R² of 0.0387, possibly due to insufficient data for the more complex architecture or the nature of the regression problem.</span>
<span id="cb14-426"><a href="#cb14-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-427"><a href="#cb14-427" aria-hidden="true" tabindex="-1"></a>Third, the engineered feature set of 25 features after colinearity removal enables the models to learn meaningful patterns, as demonstrated by positive R² values and moderate Spearman correlations around 0.33-0.34. This suggests that sentence-level features can successfully predict CCMatrix alignment scores, though the relationship remains challenging with substantial room for improvement. The moderate correlations indicate that while the models capture meaningful patterns, additional features, larger datasets, or more sophisticated architectures may further enhance performance.</span>
<span id="cb14-428"><a href="#cb14-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-429"><a href="#cb14-429" aria-hidden="true" tabindex="-1"></a><span class="fu">### Challenges and Limitations</span></span>
<span id="cb14-430"><a href="#cb14-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-431"><a href="#cb14-431" aria-hidden="true" tabindex="-1"></a>Several challenges and limitations affect the interpretation of results. First, label quality represents a significant concern, as CCMatrix alignment scores are computed automatically and may not perfectly reflect translation quality. These scores measure semantic similarity rather than true translation quality, which may introduce systematic biases in the target variable.</span>
<span id="cb14-432"><a href="#cb14-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-433"><a href="#cb14-433" aria-hidden="true" tabindex="-1"></a>Second, feature limitations constrain model performance. Sentence-level features may not capture discourse-level or context-dependent quality factors that influence translation quality. Cross-lingual embeddings may not fully capture semantic equivalence, particularly for languages with different writing systems and cultural contexts. Perplexity measures fluency but not necessarily translation accuracy, as fluent text may still contain translation errors.</span>
<span id="cb14-434"><a href="#cb14-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-435"><a href="#cb14-435" aria-hidden="true" tabindex="-1"></a>The current experiment utilized 100,000 training examples, which provided sufficient data for the models to learn robust patterns as evidenced by positive R² values and moderate correlations. This dataset size represents a substantial improvement over smaller experiments and demonstrates the importance of adequate training data for this task. Further increases to 200,000 or more pairs may provide additional improvements, though the current results suggest diminishing returns may be expected.</span>
<span id="cb14-436"><a href="#cb14-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-437"><a href="#cb14-437" aria-hidden="true" tabindex="-1"></a>The evaluation metrics demonstrate successful model learning with positive R² values and moderate correlations. The Residual MLP achieves the best performance with R² of 0.1475 and Spearman correlation of 0.3345, indicating that the models successfully learn useful patterns from the engineered features. The moderate correlation values suggest that while the models capture meaningful relationships, there remains substantial unexplained variance that could potentially be addressed through additional features or more sophisticated architectures.</span>
<span id="cb14-438"><a href="#cb14-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-439"><a href="#cb14-439" aria-hidden="true" tabindex="-1"></a><span class="fu">## Comparison with Literature</span></span>
<span id="cb14-440"><a href="#cb14-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-441"><a href="#cb14-441" aria-hidden="true" tabindex="-1"></a>Translation quality estimation has been extensively studied in the literature, with state-of-the-art methods achieving Spearman correlations of 0.6-0.8 on human-annotated data in WMT Quality Estimation Shared Tasks. Reference-free methods typically achieve lower correlations of 0.3-0.5 when using only source-target features, as they lack the information provided by reference translations.</span>
<span id="cb14-442"><a href="#cb14-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-443"><a href="#cb14-443" aria-hidden="true" tabindex="-1"></a>Our results, with Spearman correlations of approximately 0.33-0.34, represent meaningful progress toward these benchmarks, though they remain below state-of-the-art performance. The positive R² values of 0.14-0.15 demonstrate that the models successfully learn from sentence-level features, representing a substantial improvement over baseline performance. Several factors may explain the gap to state-of-the-art: CCMatrix scores may be noisier than human-annotated quality scores, as they are computed automatically using embedding-based methods. Additional features or architectures may be needed, such as pre-trained multilingual models fine-tuned specifically for quality estimation. While the current dataset of 100,000 samples enables learning, even larger training datasets or more sophisticated feature engineering may further improve performance.</span>
<span id="cb14-444"><a href="#cb14-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-445"><a href="#cb14-445" aria-hidden="true" tabindex="-1"></a><span class="fu">## Future Directions</span></span>
<span id="cb14-446"><a href="#cb14-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-447"><a href="#cb14-447" aria-hidden="true" tabindex="-1"></a><span class="fu">### Immediate Improvements</span></span>
<span id="cb14-448"><a href="#cb14-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-449"><a href="#cb14-449" aria-hidden="true" tabindex="-1"></a>Several immediate improvements could enhance model performance. Training on larger datasets using the full dataset of 200,000 or more pairs could improve generalization by providing more diverse examples. Hyperparameter tuning through grid search or Bayesian optimization could identify optimal learning rates, batch sizes, and architecture configurations. Feature engineering could be extended to include part-of-speech tag distributions, named entity overlap, word alignment scores, and translation probability from small neural machine translation models.</span>
<span id="cb14-450"><a href="#cb14-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-451"><a href="#cb14-451" aria-hidden="true" tabindex="-1"></a><span class="fu">### Advanced Methods</span></span>
<span id="cb14-452"><a href="#cb14-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-453"><a href="#cb14-453" aria-hidden="true" tabindex="-1"></a>Advanced methods could significantly improve performance. Fine-tuning pre-trained models such as mBERT or XLM-RoBERTa directly for quality estimation could leverage transfer learning from large-scale multilingual pre-training. Siamese networks that encode source and target separately before comparing representations could capture cross-lingual relationships more effectively. Ensemble methods combining predictions from multiple models through voting or stacking could improve robustness. Cross-validation using k-fold validation could provide more robust evaluation and reduce variance in performance estimates.</span>
<span id="cb14-454"><a href="#cb14-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-455"><a href="#cb14-455" aria-hidden="true" tabindex="-1"></a><span class="fu">### Evaluation Improvements</span></span>
<span id="cb14-456"><a href="#cb14-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-457"><a href="#cb14-457" aria-hidden="true" tabindex="-1"></a>Evaluation improvements would enhance understanding of model behavior. Human evaluation through creation of a small human-annotated test set could validate predictions against ground truth quality judgments. Error analysis systematically examining high-error cases could identify patterns and failure modes. Feature importance analysis using SHAP values or permutation importance could understand feature contributions. Visualization through scatter plots, error distributions, and qualitative examples could provide intuitive understanding of model performance.</span>
<span id="cb14-458"><a href="#cb14-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-459"><a href="#cb14-459" aria-hidden="true" tabindex="-1"></a><span class="fu"># Conclusion</span></span>
<span id="cb14-460"><a href="#cb14-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-461"><a href="#cb14-461" aria-hidden="true" tabindex="-1"></a><span class="fu">## Summary</span></span>
<span id="cb14-462"><a href="#cb14-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-463"><a href="#cb14-463" aria-hidden="true" tabindex="-1"></a>This project developed and evaluated deep learning models for translation quality estimation of Hindi-Chinese sentence pairs using a dataset of 100,000 sentence pairs. We engineered 31 features across five categories including length, semantic, alignment, interaction, and statistical properties, reduced to 25 features after colinearity removal, and systematically compared four neural network architectures: Simple MLP, Deep MLP, Residual MLP, and Cross-Attention Transformer.</span>
<span id="cb14-464"><a href="#cb14-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-465"><a href="#cb14-465" aria-hidden="true" tabindex="-1"></a><span class="fu">## Key Contributions</span></span>
<span id="cb14-466"><a href="#cb14-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-467"><a href="#cb14-467" aria-hidden="true" tabindex="-1"></a>The project makes several key contributions to the field of translation quality estimation. First, we developed a comprehensive feature engineering framework capturing multiple aspects of sentence pairs through length ratios, semantic similarities from multiple embedding models, fluency measures, and statistical properties. Second, we conducted a systematic architecture comparison evaluating multiple deep learning approaches on the same dataset and feature set. Third, we implemented efficient training procedures with GPU optimization including mixed precision training and tf.data pipelines. Fourth, we created a practical prediction system for quality estimation that can be used in real-world applications.</span>
<span id="cb14-468"><a href="#cb14-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-469"><a href="#cb14-469" aria-hidden="true" tabindex="-1"></a><span class="fu">## Main Findings</span></span>
<span id="cb14-470"><a href="#cb14-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-471"><a href="#cb14-471" aria-hidden="true" tabindex="-1"></a>The analysis reveals that the Residual MLP achieved the best performance with the lowest MAE of 0.6430 and highest R² of 0.1475 among the tested architectures. The models successfully predict CCMatrix scores from sentence-level features, as evidenced by positive R² values and moderate Spearman correlations around 0.33-0.34. The feature engineering framework provides a solid foundation for quality estimation, and the results demonstrate that with adequate training data (100,000 samples), meaningful patterns can be learned. Further improvements may be achieved through additional features, more sophisticated architectures, or larger datasets.</span>
<span id="cb14-472"><a href="#cb14-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-473"><a href="#cb14-473" aria-hidden="true" tabindex="-1"></a><span class="fu">## Implications</span></span>
<span id="cb14-474"><a href="#cb14-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-475"><a href="#cb14-475" aria-hidden="true" tabindex="-1"></a>The results suggest several important implications for translation quality estimation research. Sentence-level features demonstrate meaningful predictive power for CCMatrix alignment scores, with the Residual MLP achieving R² of 0.1475 and Spearman correlation of 0.3345, indicating that these features capture substantial information about alignment quality. However, there remains significant room for improvement, suggesting that additional information such as context, discourse structure, or pre-trained representations may further enhance performance. More sophisticated approaches including fine-tuned pre-trained models may be necessary to achieve performance comparable to state-of-the-art methods, though the current results establish a solid baseline. The task remains challenging but provides a strong foundation for future improvements, as the framework and methodology can be extended with additional features, architectures, and evaluation methods.</span>
<span id="cb14-476"><a href="#cb14-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-477"><a href="#cb14-477" aria-hidden="true" tabindex="-1"></a><span class="fu">## Final Remarks</span></span>
<span id="cb14-478"><a href="#cb14-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-479"><a href="#cb14-479" aria-hidden="true" tabindex="-1"></a>While the current models show limited predictive power, this project establishes a framework for translation quality estimation that can be extended with larger datasets, advanced architectures such as fine-tuned transformers, additional features including linguistic and alignment-based properties, and human-annotated evaluation data. The codebase and methodology provide a solid foundation for future research in cross-lingual quality estimation, particularly for language pairs with substantial linguistic distance such as Hindi and Chinese.</span>
<span id="cb14-480"><a href="#cb14-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-481"><a href="#cb14-481" aria-hidden="true" tabindex="-1"></a><span class="fu"># Citations</span></span>
<span id="cb14-482"><a href="#cb14-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-483"><a href="#cb14-483" aria-hidden="true" tabindex="-1"></a><span class="fu">## References</span></span>
<span id="cb14-484"><a href="#cb14-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-485"><a href="#cb14-485" aria-hidden="true" tabindex="-1"></a>The following references provide context and background for this work. Schwenk et al. (2021) introduced the CCMatrix dataset, which mines billions of high-quality parallel sentences from web-crawled data using cross-lingual embeddings. Reimers and Gurevych (2019) developed Sentence-BERT, which enables efficient computation of sentence embeddings using siamese BERT networks. Feng et al. (2022) created LaBSE, a language-agnostic BERT sentence embedding model that supports over 100 languages.</span>
<span id="cb14-486"><a href="#cb14-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-487"><a href="#cb14-487" aria-hidden="true" tabindex="-1"></a>In the domain of translation quality estimation, Specia et al. (2013) provided foundational work on quality estimation for machine translation, while Fonseca et al. (2019) reported findings from the WMT 2019 Shared Task on Quality Estimation, establishing benchmarks for the field. Kim et al. (2017) developed a predictor-estimator approach using neural quality estimation based on target word prediction.</span>
<span id="cb14-488"><a href="#cb14-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-489"><a href="#cb14-489" aria-hidden="true" tabindex="-1"></a>For technical implementation, Micikevicius et al. (2018) introduced mixed precision training, which enables faster computation while maintaining numerical stability. Abadi et al. (2016) developed TensorFlow, a system for large-scale machine learning that forms the foundation for our deep learning implementations.</span>
<span id="cb14-490"><a href="#cb14-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-491"><a href="#cb14-491" aria-hidden="true" tabindex="-1"></a><span class="fu">## Software and Tools</span></span>
<span id="cb14-492"><a href="#cb14-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-493"><a href="#cb14-493" aria-hidden="true" tabindex="-1"></a>This project utilized Python 3.x as the primary programming language, TensorFlow and Keras for deep learning model development, scikit-learn for machine learning utilities and preprocessing, sentence-transformers for multilingual sentence embeddings, pandas and numpy for data manipulation and numerical computing, and matplotlib and seaborn for data visualization and analysis.</span>
<span id="cb14-494"><a href="#cb14-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-495"><a href="#cb14-495" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb14-496"><a href="#cb14-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-497"><a href="#cb14-497" aria-hidden="true" tabindex="-1"></a>*This report was generated using Quarto. The code and data are available in the project repository.*</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>