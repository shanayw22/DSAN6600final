---
title: "Translation Quality Estimation for Hindi-Chinese Sentence Pairs"
subtitle: "A Deep Learning Approach to Predict Alignment Scores Without Reference Translations"
author: "DSAN6600 Final Project"
date: today
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: show
    code-tools: true
    theme: cosmo
    fig-width: 10
    fig-height: 6
    fig-dpi: 300
bibliography: references.bib
embed-resources: true
---

::: {.callout-note}
## Rendering Instructions

To render this report, first generate training results by running `scripts/train_model.py` to create `models/training_results.pkl` and `models/training_summary.txt`. Ensure that `pandas`, `numpy`, `matplotlib`, and `seaborn` are installed. Then render with Quarto by running `quarto render report.qmd` in the project root directory. The report will automatically load and visualize training results if available.
:::

```{python}
#| echo: false
#| warning: false
#| message: false
import pandas as pd
import numpy as np
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Set style
try:
    plt.style.use('seaborn-v0_8-darkgrid')
except OSError:
    try:
        plt.style.use('seaborn-darkgrid')
    except OSError:
        plt.style.use('default')
sns.set_palette("husl")
```

# Introduction

## Problem Statement

Translation quality estimation (TQE) represents a critical task in natural language processing that aims to assess the quality of translated text without requiring reference translations. This capability is particularly valuable for machine translation systems that must automatically filter low-quality translations, for parallel corpus construction where high-quality sentence pairs must be identified for training data, for post-editing workflows that prioritize translations requiring human review, and for cross-lingual applications that evaluate alignment quality in multilingual datasets.

Traditional quality estimation methods often rely on reference translations, which are expensive and time-consuming to obtain. This project addresses the challenge of predicting translation quality scores for Hindi-Chinese sentence pairs using only source-target features, without access to reference translations.

## Motivation

Hindi and Chinese represent two of the world's most widely spoken languages, yet they belong to fundamentally different language families—Indo-European versus Sino-Tibetan—with distinct writing systems, grammatical structures, and cultural contexts. This substantial linguistic distance makes quality estimation particularly challenging but also scientifically interesting, as it tests the limits of cross-lingual feature extraction and neural network learning.

The CCMatrix dataset provides alignment scores that reflect semantic similarity between sentence pairs, serving as a proxy for translation quality. By learning to predict these scores from sentence-level features, we can develop a system that automates quality assessment, reduces the need for manual evaluation, scales to large corpora processing millions of sentence pairs efficiently, provides interpretable features using linguistically motivated characteristics that can be analyzed, and generalizes across domains by learning patterns that may transfer to other language pairs.

## Research Questions

This project addresses four primary research questions. First, can we predict translation quality scores from source-target features alone? We investigate whether sentence-level features including length, semantic similarity, and fluency are sufficient to predict alignment scores. Second, which features are most predictive of translation quality? We analyze the contribution of different feature categories including length, semantic, and statistical properties to model performance. Third, how do different neural network architectures compare for this regression task? We systematically compare Simple MLP, Deep MLP, Residual MLP, and Transformer architectures. Fourth, what is the relationship between predicted scores and actual CCMatrix alignment scores? We evaluate correlation metrics including Pearson and Spearman correlations to assess prediction quality.

## Objectives

The primary objectives of this project are to develop a regression model that predicts continuous CCMatrix alignment scores in the 0.0-1.0 range, to engineer comprehensive features capturing length, semantic, statistical, and interaction properties, to compare multiple deep learning architectures to identify the best-performing model, to evaluate model performance using multiple metrics including MAE, RMSE, R², and correlation coefficients, and to provide a practical tool for quality estimation that can be used in real-world applications.

# Methods

## Dataset

### Data Source

The project utilizes the CCMatrix Hindi-Chinese parallel corpus, which contains sentence pairs extracted from web-crawled multilingual data. CCMatrix employs a cross-lingual embedding approach to compute alignment scores that reflect semantic similarity between sentence pairs. The dataset originates from the CC-Aligned project and consists of three files: `CCMatrix.hi-zh.hi` containing Hindi sentences with one sentence per line, `CCMatrix.hi-zh.zh` containing Chinese sentences with one sentence per line, and `CCMatrix.hi-zh.scores` containing alignment scores as continuous values with one score per line.

### Data Preprocessing

```{python}
#| echo: false
#| warning: false
# Load training summary to get dataset info
try:
    with open('models/training_summary.txt', 'r') as f:
        summary_text = f.read()
    print("Dataset Information from Training Summary:")
    print("=" * 70)
    print(summary_text)
except FileNotFoundError:
    print("Training summary not found. Please run train_model.py first.")
```

The dataset preprocessing pipeline involves four main steps. First, sentence pairing matches Hindi and Chinese sentences by line number to ensure correspondence. Second, score extraction retrieves alignment scores from the scores file. Third, sampling applies a configurable sample size for training, with the current experiment utilizing 100,000 sentence pairs to provide sufficient data for robust model learning. Fourth, the data is split into training, validation, and test sets using a 70% / 15% / 15% partition, resulting in 70,000 training samples, 15,000 validation samples, and 15,000 test samples for model training and evaluation.

### Target Variable

The CCMatrix alignment score represents a continuous value typically ranging from 0.0 to 1.0, where higher scores closer to 1.0 indicate better alignment and semantic similarity, while lower scores closer to 0.0 indicate poor alignment or unrelated sentences. These scores are computed using cross-lingual embeddings and serve as a proxy for translation quality, though they measure semantic similarity rather than true translation accuracy.

## Feature Engineering

We engineer over 32 features across five distinct categories, each designed to capture different aspects of sentence pair relationships.

### Length Features

The first category encompasses nine length features that capture the relationship between sentence lengths in both languages. These include `hindi_length` representing character count in Hindi sentences, `chinese_length` representing character count in Chinese sentences, `hindi_word_count` representing word count in Hindi sentences, `chinese_char_count` representing Chinese character count specifically for Han characters, `length_ratio` representing the ratio of Hindi to Chinese length, `length_diff` representing absolute difference in lengths, `hindi_avg_word_length` representing average word length in Hindi, `chinese_avg_char_length` representing average character length in Chinese, and `length_ratio_normalized` representing normalized length ratio. The rationale for these features stems from the observation that length mismatches can indicate translation quality issues, as well-aligned translations typically exhibit proportional lengths across languages.

### Semantic Features

The second category comprises eleven semantic features that capture semantic similarity and fluency. These include `embedding_similarity` computed as cosine similarity using the `paraphrase-multilingual-MiniLM-L12-v2` model, `labse_similarity` computed as cosine similarity using the `sentence-transformers/LaBSE` model, `chinese_perplexity` representing language model perplexity as a fluency indicator, `chinese_fluency` computed as the inverse of perplexity, `embedding_labse_interaction` representing the product of embedding and LaBSE similarities, `similarity_avg` representing the average of embedding and LaBSE similarities, `similarity_diff` representing the absolute difference between similarities, `embedding_similarity_squared` representing squared embedding similarity as a polynomial feature, `labse_similarity_squared` representing squared LaBSE similarity as a polynomial feature, and `similarity_ratio` representing the ratio of embedding to LaBSE similarity. The rationale for these features is that semantic similarity serves as a strong indicator of translation quality, and using multiple embedding models provides complementary signals that may capture different aspects of semantic equivalence.

### Alignment Features

The third category contains a single alignment feature: `char_alignment` computed as a character-level alignment heuristic using Jaccard similarity of character sets. This feature captures surface-level similarity that may not be captured by semantic embeddings, particularly for languages with different writing systems where character overlap might indicate transliteration or proper noun correspondence.

### Interaction Features

The fourth category includes three interaction features that combine length and semantic information: `length_similarity_interaction` representing the product of length ratio and embedding similarity, `length_perplexity_interaction` representing the product of Chinese length and perplexity, and `length_perplexity_ratio` representing the ratio of Chinese length to perplexity. These interaction features can capture non-linear relationships between features that may not be apparent in individual feature values.

### Statistical Features

The fifth category encompasses eight statistical features that capture text characteristics: `punctuation_count_hi` and `punctuation_count_zh` representing punctuation counts in Hindi and Chinese respectively, `digit_count_hi` and `digit_count_zh` representing digit counts in both languages, `punctuation_ratio_hi` and `punctuation_ratio_zh` representing punctuation ratios normalized by sentence length, and `vocab_diversity_hi` and `vocab_diversity_zh` representing vocabulary diversity computed as the ratio of unique tokens to total tokens. These statistical properties can indicate translation quality and text characteristics that may correlate with alignment scores.

### Feature Selection

After feature engineering, we apply colinearity removal to eliminate highly correlated features with correlation coefficients exceeding 0.95. This process reduces redundancy and improves model generalization by preventing multicollinearity issues. The feature selection step reduced the feature set from 31 to 25 features in the current experiment, removing 6 highly correlated features including embedding similarity squared, Hindi word count, LaBSE similarity squared, length difference, normalized length ratio, and similarity average.

## Model Architectures

We compare four deep learning architectures, each representing different approaches to learning from the engineered features.

### Simple MLP

The first architecture is a baseline multi-layer perceptron with regularization. The architecture consists of an input layer followed by a dense layer with 128 units and ReLU activation, dropout with rate 0.3, a second dense layer with 64 units and ReLU activation, dropout with rate 0.2, and a final dense output layer with a single linear unit. Regularization includes L2 weight decay with coefficient 0.01 and dropout layers. This model contains approximately 20,000 parameters and serves as a baseline for comparison.

### Deep MLP

The second architecture is a deeper network with batch normalization. The architecture consists of an input layer followed by a dense layer with 256 units and ReLU activation, batch normalization, dropout with rate 0.4, a second dense layer with 128 units and ReLU activation, batch normalization, dropout with rate 0.3, a third dense layer with 64 units and ReLU activation, dropout with rate 0.2, and a final dense output layer with a single linear unit. Regularization includes L2 weight decay with coefficient 0.01, batch normalization layers, and dropout with rates ranging from 0.2 to 0.4. This model contains approximately 100,000 parameters.

### Residual MLP

The third architecture is a residual network with skip connections. The architecture is similar to the Deep MLP but incorporates residual connections between layers, allowing gradients to flow more easily through the network and potentially enabling training of deeper architectures. Regularization includes L2 weight decay with coefficient 0.01, batch normalization layers, and dropout with rates ranging from 0.2 to 0.4. This model contains approximately 120,000 parameters.

### Cross-Attention Transformer

The fourth architecture is a transformer model with cross-attention between Hindi and Chinese embeddings. The architecture includes separate input projections for Hindi and Chinese embeddings, multi-head cross-attention layers enabling bidirectional attention between Hindi and Chinese representations, feed-forward networks with layer normalization, and concatenation followed by final dense layers. Hyperparameters include 8 attention heads, 2 transformer layers, and a model dimension of 256. This model contains approximately 200,000 parameters and represents the most sophisticated architecture tested.

## Training Procedure

### Data Preprocessing

The training procedure begins with comprehensive data preprocessing. Feature scaling applies StandardScaler to all features, transforming them to have zero mean and unit variance. Target scaling applies StandardScaler to CCMatrix scores to improve neural network training stability. During evaluation, predictions are inverse-transformed back to the original scale to compute metrics on the natural score range.

### Training Configuration

Training employs the Adam optimizer with default learning rate settings. The loss function is Mean Squared Error (MSE), with Mean Absolute Error (MAE) tracked as an additional metric. Batch size is set to 256 for GPU training or 64 for CPU training, depending on hardware availability. Training proceeds for up to 100 epochs with early stopping based on validation loss. Early stopping patience is set to 10 epochs for MLP architectures and 5 epochs for the transformer architecture. Mixed precision training with FP16 is enabled on GPU for faster computation while maintaining numerical stability in the output layer through FP32 precision.

### GPU Optimization

The training script incorporates several GPU optimizations to maximize computational efficiency. Memory growth enables dynamic GPU memory allocation to avoid allocating all GPU memory at once. Graph mode utilizes TensorFlow graph execution for faster computation compared to eager execution. Mixed precision training employs FP16 for intermediate computations while maintaining FP32 for the output layer. The tf.data pipeline provides efficient data loading with prefetching and batching to minimize data loading bottlenecks during training.

### Evaluation Metrics

Models are evaluated using multiple metrics to provide comprehensive performance assessment. Mean Absolute Error (MAE) measures average prediction error and is the primary metric for model selection. Root Mean Squared Error (RMSE) penalizes larger errors more heavily than smaller errors. The coefficient of determination (R²) measures the proportion of variance in the target variable explained by the model. Pearson correlation measures linear relationship strength between predictions and actual scores. Spearman correlation measures monotonic relationship strength using rank-based correlation, which is less sensitive to outliers than Pearson correlation.

### Cross Validation(800k)

To identify the best-performing architecture and hyperparameters at the 800k scale, we performed a structured hyperparameter search combined with 5-fold cross-validation. Three model families were evaluated: MLP, Residual MLP, and Transformer. Each model was trained and validated using identical folds to ensure fair comparison.

| Model                              | Hyperparameters                |
| ---------------------------------- | ------------------------------ |
| **MLP**                            | Hidden Dim: **[64, 128, 256]** |
| Num Layers: **[2, 3, 4]**          |                                |
| Dropout: **[0.2, 0.3]**            |                                |
| Learning Rate: **[0.001, 0.0005]** |                                |
| **Residual MLP**                   | Hidden Dim: **[64, 128, 256]** |
| Num Layers: **[2, 3]**             |                                |
| Dropout: **[0.2, 0.3]**            |                                |
| Learning Rate: **[0.001, 0.0005]** |                                |
| **Transformer**                    | Model Dim: **[64, 128]**       |
| Num Heads: **[4, 8]**              |                                |
| Dropout: **[0.1, 0.2]**            |                                |
| Learning Rate: **[0.001, 0.0005]** |                                |

The search explored model capacity, depth, regularization strength, and optimization rates.
Best configurations were selected using lowest mean CV RMSE.

### Cross-Validation Results (Best Hyperparameters)

| Model        | Best Hyperparameters                                              | Best CV RMSE |
|--------------|-------------------------------------------------------------------|--------------|
| **MLP**         | Hidden Dim **256**, Layers **3**, Dropout **0.2**, LR **0.0005**          | **0.01421351** |
| **Residual MLP** | Hidden Dim **256**, Layers **3**, Dropout **0.2**, LR **0.0005**          | **0.01421057** |
| **Transformer**  | Model Dim **64**, Heads **8**, Dropout **0.1**, LR **0.0005**             | **0.01425524** |

Residual MLP achieved the best average CV performance, though all architectures performed similarly within a narrow RMSE range.

### Final Test-Set Performance

Using the best hyperparameters from CV, final models were retrained on the full training data and evaluated on a held-out test set.

| Model          | RMSE        | MAE        | R²        |
|----------------|-------------|------------|-----------|
| **MLP**          | **0.014317** | **0.009913** | **0.154002** |
| **Residual MLP** | **0.014277** | **0.009913** | **0.158780** |
| **Transformer**  | **0.014353** | **0.009926** | **0.149814** |

Across all metrics, the **Residual MLP** delivered the strongest generalization performance, achieving the highest R² and lowest RMSE on the test set.

### Summary

Cross-validation demonstrated that:

- All three architectures perform competitively on the engineered feature set.
- The **Residual MLP** provided the best combination of stability and accuracy.
- Hyperparameter tuning (especially model depth, width, and dropout) significantly impacts performance at the 800k data scale.

This CV stage establishes the foundation for selecting the final model configuration used in downstream evaluation and SHAP interpretability analysis.

# Analyses

## Data Exploration

```{python}
#| echo: true
#| warning: false
#| fig-cap: "Distribution of CCMatrix alignment scores in the dataset"
#| fig-width: 10
#| fig-height: 6

# Load training results
try:
    with open('models/training_results.pkl', 'rb') as f:
        results = pickle.load(f)
    
    # Load training summary
    with open('models/training_summary.txt', 'r') as f:
        summary_lines = f.readlines()
    
    print("Training Results Summary:")
    print("=" * 70)
    for model_name, metrics in results.items():
        print(f"\n{model_name.upper()}:")
        print(f"  Test MAE: {metrics['test_mae']:.4f}")
        print(f"  Test RMSE: {metrics['test_rmse']:.4f}")
        print(f"  Test R²: {metrics['test_r2']:.4f}")
        if metrics.get('test_spearman') is not None:
            print(f"  Test Spearman: {metrics['test_spearman']:.4f}")
        if metrics.get('test_pearson') is not None:
            print(f"  Test Pearson: {metrics['test_pearson']:.4f}")
    
    # Create comparison plot
    models = list(results.keys())
    mae_values = [results[m]['test_mae'] for m in models]
    rmse_values = [results[m]['test_rmse'] for m in models]
    r2_values = [results[m]['test_r2'] for m in models]
    spearman_values = [results[m].get('test_spearman', 0) if results[m].get('test_spearman') is not None else 0 for m in models]
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # MAE comparison
    axes[0, 0].bar(models, mae_values, color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12'])
    axes[0, 0].set_title('Test MAE by Model', fontsize=14, fontweight='bold')
    axes[0, 0].set_ylabel('MAE', fontsize=12)
    axes[0, 0].tick_params(axis='x', rotation=45)
    axes[0, 0].grid(axis='y', alpha=0.3)
    
    # RMSE comparison
    axes[0, 1].bar(models, rmse_values, color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12'])
    axes[0, 1].set_title('Test RMSE by Model', fontsize=14, fontweight='bold')
    axes[0, 1].set_ylabel('RMSE', fontsize=12)
    axes[0, 1].tick_params(axis='x', rotation=45)
    axes[0, 1].grid(axis='y', alpha=0.3)
    
    # R² comparison
    axes[1, 0].bar(models, r2_values, color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12'])
    axes[1, 0].set_title('Test R² by Model', fontsize=14, fontweight='bold')
    axes[1, 0].set_ylabel('R²', fontsize=12)
    axes[1, 0].tick_params(axis='x', rotation=45)
    axes[1, 0].grid(axis='y', alpha=0.3)
    axes[1, 0].axhline(y=0, color='r', linestyle='--', alpha=0.5)
    
    # Spearman correlation comparison
    axes[1, 1].bar(models, spearman_values, color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12'])
    axes[1, 1].set_title('Test Spearman Correlation by Model', fontsize=14, fontweight='bold')
    axes[1, 1].set_ylabel('Spearman Correlation', fontsize=12)
    axes[1, 1].tick_params(axis='x', rotation=45)
    axes[1, 1].grid(axis='y', alpha=0.3)
    axes[1, 1].axhline(y=0, color='r', linestyle='--', alpha=0.5)
    
    plt.tight_layout()
    plt.show()
    
except FileNotFoundError:
    print("Training results not found. Please run train_model.py first to generate results.")
```

## Model Performance Comparison(100k)

```{python}
#| echo: true
#| warning: false
#| fig-cap: "Comprehensive model performance comparison across all metrics"
#| fig-width: 12
#| fig-height: 8

try:
    # Create a comprehensive comparison table
    comparison_data = []
    for model_name, metrics in results.items():
        comparison_data.append({
            'Model': model_name.replace('_', ' ').title(),
            'MAE': metrics['test_mae'],
            'RMSE': metrics['test_rmse'],
            'R²': metrics['test_r2'],
            'Spearman': metrics.get('test_spearman', np.nan),
            'Pearson': metrics.get('test_pearson', np.nan),
            'Train MAE': metrics.get('train_mae', np.nan),
            'Val MAE': metrics.get('val_mae', np.nan)
        })
    
    df_comparison = pd.DataFrame(comparison_data)
    df_comparison = df_comparison.sort_values('MAE')
    
    print("\nModel Performance Comparison Table:")
    print("=" * 100)
    print(df_comparison.to_string(index=False))
    
    # Create heatmap of metrics
    metrics_for_heatmap = df_comparison.set_index('Model')[['MAE', 'RMSE', 'R²', 'Spearman', 'Pearson']]
    
    fig, ax = plt.subplots(figsize=(10, 6))
    sns.heatmap(metrics_for_heatmap.T, annot=True, fmt='.4f', cmap='RdYlGn_r', 
                center=0, vmin=-0.2, vmax=1.2, cbar_kws={'label': 'Metric Value'}, ax=ax)
    ax.set_title('Model Performance Heatmap', fontsize=16, fontweight='bold', pad=20)
    ax.set_xlabel('Model', fontsize=12)
    ax.set_ylabel('Metric', fontsize=12)
    plt.tight_layout()
    plt.show()
    
except NameError:
    print("Results not loaded. Please run the previous code chunk first.")
```

## Training Curves

```{python}
#| echo: true
#| warning: false
#| fig-cap: "Training and validation loss curves for all models (if available)"
#| fig-width: 12
#| fig-height: 8

# Note: Training history is not saved by default in the current implementation
# This is a placeholder for when training history is available
print("Note: Training history (loss curves) would be displayed here if saved during training.")
print("To enable this, modify train_model.py to save training history dictionaries.")
print("\nCurrent implementation saves final metrics but not epoch-by-epoch training curves.")
```

## Best Model Analysis

```{python}
#| echo: true
#| warning: false

try:
    # Find best model (lowest MAE)
    best_model_name = min(results.keys(), key=lambda k: results[k]['test_mae'])
    best_metrics = results[best_model_name]
    
    print("=" * 70)
    print("BEST MODEL ANALYSIS")
    print("=" * 70)
    print(f"\nBest Model: {best_model_name.upper()}")
    print(f"\nPerformance Metrics:")
    print(f"  Test MAE:  {best_metrics['test_mae']:.4f}")
    print(f"  Test RMSE: {best_metrics['test_rmse']:.4f}")
    print(f"  Test R²:   {best_metrics['test_r2']:.4f}")
    if best_metrics.get('test_spearman') is not None:
        print(f"  Test Spearman: {best_metrics['test_spearman']:.4f}")
    if best_metrics.get('test_pearson') is not None:
        print(f"  Test Pearson:  {best_metrics['test_pearson']:.4f}")
    
    print(f"\nTraining Metrics:")
    train_mae = best_metrics.get('train_mae')
    val_mae = best_metrics.get('val_mae')
    print(f"  Train MAE: {train_mae:.4f}" if train_mae is not None else "  Train MAE: N/A")
    print(f"  Val MAE:   {val_mae:.4f}" if val_mae is not None else "  Val MAE:   N/A")
    
    # Interpretation
    print(f"\nInterpretation:")
    if best_metrics['test_r2'] > 0:
        print(f"  ✓ The model explains {best_metrics['test_r2']*100:.1f}% of variance in CCMatrix scores")
    else:
        print(f"  ⚠ The model performs worse than a simple mean baseline (R² < 0)")
    
    if best_metrics.get('test_spearman') is not None:
        if abs(best_metrics['test_spearman']) > 0.5:
            strength = "strong" if abs(best_metrics['test_spearman']) > 0.7 else "moderate"
            print(f"  ✓ {strength.capitalize()} monotonic relationship (Spearman = {best_metrics['test_spearman']:.3f})")
        else:
            print(f"  ⚠ Weak monotonic relationship (Spearman = {best_metrics['test_spearman']:.3f})")
    
except NameError:
    print("Results not loaded. Please run the previous code chunks first.")
```

## Feature Importance Analysis(SHAP 800K)

### Key Findings

::: {layout-ncol=3}

![](scripts/v2/V2_Shap_Result/shap_bar_mlp.png){fig-alt="MLP SHAP bar plot" width="100%"}

![](scripts/v2/V2_Shap_Result/shap_bar_residual_mlp.png){fig-alt="Residual MLP SHAP bar plot" width="100%"}

![](scripts/v2/V2_Shap_Result/shap_bar_transformer.png){fig-alt="Transformer SHAP bar plot" width="100%"}

:::

::: {layout-ncol=3}

![](scripts/v2/V2_Shap_Result/shap_summary_mlp.png){fig-alt="MLP SHAP summary" width="100%"}

![](scripts/v2/V2_Shap_Result/shap_summary_residual_mlp.png){fig-alt="Residual MLP SHAP summary" width="100%"}

![](scripts/v2/V2_Shap_Result/shap_summary_transformer.png){fig-alt="Transformer SHAP summary" width="100%"}

:::

- **Semantic interaction features dominate across all models.**  
  `embedding_labse_interaction` and `labse_similarity` consistently appear as the top contributors, confirming that semantic alignment between Hindi and Chinese sentences is the strongest predictor of CCMatrix scores.

- **Character-based length indicators also play a major role.**  
  Features such as `chinese_char_count`, `hindi_length`, and `chinese_length` rank highly, indicating that proportional sentence length remains a strong signal of translation quality.

- **Residual MLP demonstrates more stable feature importance ordering.**  
  Its SHAP rankings show less fluctuation compared to MLP and Transformer, highlighting the stabilizing effect of residual connections.

- **Transformer relies more heavily on pure semantic similarity.**  
  For the Transformer, `labse_similarity` alone contributes more than any interaction term, reflecting its stronger capacity to capture high-level semantic relationships.

- **Statistical features contribute moderately but consistently.**  
  Features such as punctuation ratios, vocabulary diversity, and digit counts rank mid-to-low, suggesting they complement semantic indicators rather than drive predictions.

### Interpretation

Across models, semantic similarity remains the most influential feature category, followed by length-based alignment and statistical diagnostics.  
Residual MLP offers the most balanced pattern of attribution, supporting its superior generalization performance observed in evaluation metrics.

The SHAP analysis confirms that the engineered feature set captures meaningful structure in translation quality, and the model behaviors align with linguistic intuition.

# Discussion

## Model Performance Interpretation

### Overall Performance

The models achieve improved performance on the translation quality estimation task when trained on the larger dataset. The best model, identified as Residual MLP, achieves a mean absolute error of 0.6430 on the CCMatrix score scale, a positive R² value of 0.1475 indicating that the model explains approximately 14.75% of variance in alignment scores, and a Spearman correlation of 0.3345 indicating a moderate monotonic relationship between predictions and actual scores. These results represent substantial improvement over baseline performance and demonstrate that the models successfully learn meaningful patterns from the engineered features.

### Key Findings

The analysis reveals several important findings. First, the models demonstrate meaningful predictive power when trained on the larger dataset of 100,000 samples, as evidenced by positive R² values of approximately 0.14-0.15, indicating that the models explain a significant portion of variance in CCMatrix alignment scores. This represents a substantial improvement over baseline performance and demonstrates that sentence-level features can successfully predict alignment scores, though the relationship remains challenging with room for further improvement.

Second, the model architecture comparison reveals that the Residual MLP performs best with the lowest MAE of 0.6430 and highest R² of 0.1475, suggesting that residual connections enable more effective learning of complex patterns in the larger dataset. The Deep MLP achieves similar performance with MAE of 0.6475 and R² of 0.1442, while the Simple MLP provides a reasonable baseline with MAE of 0.6488 and R² of 0.1446. The Transformer architecture achieves lower performance with MAE of 0.6846 and R² of 0.0387, possibly due to insufficient data for the more complex architecture or the nature of the regression problem.

Third, the engineered feature set of 25 features after colinearity removal enables the models to learn meaningful patterns, as demonstrated by positive R² values and moderate Spearman correlations around 0.33-0.34. This suggests that sentence-level features can successfully predict CCMatrix alignment scores, though the relationship remains challenging with substantial room for improvement. The moderate correlations indicate that while the models capture meaningful patterns, additional features, larger datasets, or more sophisticated architectures may further enhance performance.

### Challenges and Limitations

Several challenges and limitations affect the interpretation of results. First, label quality represents a significant concern, as CCMatrix alignment scores are computed automatically and may not perfectly reflect translation quality. These scores measure semantic similarity rather than true translation quality, which may introduce systematic biases in the target variable.

Second, feature limitations constrain model performance. Sentence-level features may not capture discourse-level or context-dependent quality factors that influence translation quality. Cross-lingual embeddings may not fully capture semantic equivalence, particularly for languages with different writing systems and cultural contexts. Perplexity measures fluency but not necessarily translation accuracy, as fluent text may still contain translation errors.

The current experiment utilized 100,000 training examples, which provided sufficient data for the models to learn robust patterns as evidenced by positive R² values and moderate correlations. This dataset size represents a substantial improvement over smaller experiments and demonstrates the importance of adequate training data for this task. Further increases to 200,000 or more pairs may provide additional improvements, though the current results suggest diminishing returns may be expected.

The evaluation metrics demonstrate successful model learning with positive R² values and moderate correlations. The Residual MLP achieves the best performance with R² of 0.1475 and Spearman correlation of 0.3345, indicating that the models successfully learn useful patterns from the engineered features. The moderate correlation values suggest that while the models capture meaningful relationships, there remains substantial unexplained variance that could potentially be addressed through additional features or more sophisticated architectures.

## Comparison with Literature

Translation quality estimation has been extensively studied in the literature, with state-of-the-art methods achieving Spearman correlations of 0.6-0.8 on human-annotated data in WMT Quality Estimation Shared Tasks. Reference-free methods typically achieve lower correlations of 0.3-0.5 when using only source-target features, as they lack the information provided by reference translations.

Our results, with Spearman correlations of approximately 0.33-0.34, represent meaningful progress toward these benchmarks, though they remain below state-of-the-art performance. The positive R² values of 0.14-0.15 demonstrate that the models successfully learn from sentence-level features, representing a substantial improvement over baseline performance. Several factors may explain the gap to state-of-the-art: CCMatrix scores may be noisier than human-annotated quality scores, as they are computed automatically using embedding-based methods. Additional features or architectures may be needed, such as pre-trained multilingual models fine-tuned specifically for quality estimation. While the current dataset of 100,000 samples enables learning, even larger training datasets or more sophisticated feature engineering may further improve performance.

## Future Directions

### Immediate Improvements

Several immediate improvements could enhance model performance. Training on larger datasets using the full dataset of 200,000 or more pairs could improve generalization by providing more diverse examples. Hyperparameter tuning through grid search or Bayesian optimization could identify optimal learning rates, batch sizes, and architecture configurations. Feature engineering could be extended to include part-of-speech tag distributions, named entity overlap, word alignment scores, and translation probability from small neural machine translation models.

### Advanced Methods

Advanced methods could significantly improve performance. Fine-tuning pre-trained models such as mBERT or XLM-RoBERTa directly for quality estimation could leverage transfer learning from large-scale multilingual pre-training. Siamese networks that encode source and target separately before comparing representations could capture cross-lingual relationships more effectively. Ensemble methods combining predictions from multiple models through voting or stacking could improve robustness. Cross-validation using k-fold validation could provide more robust evaluation and reduce variance in performance estimates.

### Evaluation Improvements

Evaluation improvements would enhance understanding of model behavior. Human evaluation through creation of a small human-annotated test set could validate predictions against ground truth quality judgments. Error analysis systematically examining high-error cases could identify patterns and failure modes. Feature importance analysis using SHAP values or permutation importance could understand feature contributions. Visualization through scatter plots, error distributions, and qualitative examples could provide intuitive understanding of model performance.

# Conclusion

## Summary

This project developed and evaluated deep learning models for translation quality estimation of Hindi-Chinese sentence pairs using a dataset of 100,000 sentence pairs. We engineered 31 features across five categories including length, semantic, alignment, interaction, and statistical properties, reduced to 25 features after colinearity removal, and systematically compared four neural network architectures: Simple MLP, Deep MLP, Residual MLP, and Cross-Attention Transformer.

## Key Contributions

The project makes several key contributions to the field of translation quality estimation. First, we developed a comprehensive feature engineering framework capturing multiple aspects of sentence pairs through length ratios, semantic similarities from multiple embedding models, fluency measures, and statistical properties. Second, we conducted a systematic architecture comparison evaluating multiple deep learning approaches on the same dataset and feature set. Third, we implemented efficient training procedures with GPU optimization including mixed precision training and tf.data pipelines. Fourth, we created a practical prediction system for quality estimation that can be used in real-world applications.

## Main Findings

The analysis reveals that the Residual MLP achieved the best performance with the lowest MAE of 0.6430 and highest R² of 0.1475 among the tested architectures. The models successfully predict CCMatrix scores from sentence-level features, as evidenced by positive R² values and moderate Spearman correlations around 0.33-0.34. The feature engineering framework provides a solid foundation for quality estimation, and the results demonstrate that with adequate training data (100,000 samples), meaningful patterns can be learned. Further improvements may be achieved through additional features, more sophisticated architectures, or larger datasets.

## Implications

The results suggest several important implications for translation quality estimation research. Sentence-level features demonstrate meaningful predictive power for CCMatrix alignment scores, with the Residual MLP achieving R² of 0.1475 and Spearman correlation of 0.3345, indicating that these features capture substantial information about alignment quality. However, there remains significant room for improvement, suggesting that additional information such as context, discourse structure, or pre-trained representations may further enhance performance. More sophisticated approaches including fine-tuned pre-trained models may be necessary to achieve performance comparable to state-of-the-art methods, though the current results establish a solid baseline. The task remains challenging but provides a strong foundation for future improvements, as the framework and methodology can be extended with additional features, architectures, and evaluation methods.

## Final Remarks

While the current models show limited predictive power, this project establishes a framework for translation quality estimation that can be extended with larger datasets, advanced architectures such as fine-tuned transformers, additional features including linguistic and alignment-based properties, and human-annotated evaluation data. The codebase and methodology provide a solid foundation for future research in cross-lingual quality estimation, particularly for language pairs with substantial linguistic distance such as Hindi and Chinese.

# Citations

## References

The following references provide context and background for this work. Schwenk et al. (2021) introduced the CCMatrix dataset, which mines billions of high-quality parallel sentences from web-crawled data using cross-lingual embeddings. Reimers and Gurevych (2019) developed Sentence-BERT, which enables efficient computation of sentence embeddings using siamese BERT networks. Feng et al. (2022) created LaBSE, a language-agnostic BERT sentence embedding model that supports over 100 languages.

In the domain of translation quality estimation, Specia et al. (2013) provided foundational work on quality estimation for machine translation, while Fonseca et al. (2019) reported findings from the WMT 2019 Shared Task on Quality Estimation, establishing benchmarks for the field. Kim et al. (2017) developed a predictor-estimator approach using neural quality estimation based on target word prediction.

For technical implementation, Micikevicius et al. (2018) introduced mixed precision training, which enables faster computation while maintaining numerical stability. Abadi et al. (2016) developed TensorFlow, a system for large-scale machine learning that forms the foundation for our deep learning implementations.

## Software and Tools

This project utilized Python 3.x as the primary programming language, TensorFlow and Keras for deep learning model development, scikit-learn for machine learning utilities and preprocessing, sentence-transformers for multilingual sentence embeddings, pandas and numpy for data manipulation and numerical computing, and matplotlib and seaborn for data visualization and analysis.

---

*This report was generated using Quarto. The code and data are available in the project repository.*
